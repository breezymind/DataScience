{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LearningPySpark_Ch03\n",
    "## Chapter 4: DataFrames\n",
    "This notebook contains sample code from Chapter 4 of <a href=\"https://render.githubusercontent.com/view/ipynb?commit=edcaf5ed42558fb08759f70a01201e00aa2d49f7&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f5061636b745075626c697368696e672f4c6561726e696e672d5079537061726b2f656463616635656434323535386662303837353966373061303132303165303061613264343966372f4368617074657230322f4c6561726e696e675079537061726b5f4368617074657230332e6970796e62&nwo=PacktPublishing%2FLearning-PySpark&path=Chapter02%2FLearningPySpark_Chapter03.ipynb&repository_id=83264169\">Learning PySpark</a> focusing on PySpark and DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate your own DataFrame\n",
    "- Instead of accessing the file system, let's create a DataFrame by generating the data. \n",
    "- In this case, we'll first create the stringRDD RDD and then convert it into a DataFrame when we're reading stringJSONRDD using spark.read.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:475"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate our own JSON data \n",
    "#   This way we don't have to access the file system yet.\n",
    "stringJSONRDD = sc.parallelize((\"\"\" \n",
    "  { \"id\": \"123\",\n",
    "    \"name\": \"Katie\",\n",
    "    \"age\": 19,\n",
    "    \"eyeColor\": \"brown\"\n",
    "  }\"\"\",\n",
    "   \"\"\"{\n",
    "    \"id\": \"234\",\n",
    "    \"name\": \"Michael\",\n",
    "    \"age\": 22,\n",
    "    \"eyeColor\": \"green\"\n",
    "  }\"\"\", \n",
    "  \"\"\"{\n",
    "    \"id\": \"345\",\n",
    "    \"name\": \"Simone\",\n",
    "    \"age\": 23,\n",
    "    \"eyeColor\": \"blue\"\n",
    "  }\"\"\")\n",
    ")\n",
    "stringJSONRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\n  { \"id\": \"123\",\\n    \"name\": \"Katie\",\\n    \"age\": 19,\\n    \"eyeColor\": \"brown\"\\n  }',\n",
       " '{\\n    \"id\": \"234\",\\n    \"name\": \"Michael\",\\n    \"age\": 22,\\n    \"eyeColor\": \"green\"\\n  }',\n",
       " '{\\n    \"id\": \"345\",\\n    \"name\": \"Simone\",\\n    \"age\": 23,\\n    \"eyeColor\": \"blue\"\\n  }']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringJSONRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n",
       " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
       " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "swimmersJSON = spark.read.json(stringJSONRDD)\n",
    "swimmersJSON.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create temporary table\n",
    "swimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 19|   brown|123|  Katie|\n",
      "| 22|   green|234|Michael|\n",
      "| 23|    blue|345| Simone|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame API\n",
    "swimmersJSON.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n",
       " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
       " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL Query\n",
    "spark.sql(\"select * from swimmersJSON\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-14d9a9d63ad0>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-14d9a9d63ad0>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    -- Query Data\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%sql \n",
    "-- Query Data\n",
    "select * from swimmersJSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferring the Schema Using Reflection\n",
    "Note that Apache Spark is inferring the schema using reflection; <br>\n",
    "i.e. it automaticlaly determines the schema of the data based on reviewing the JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema\n",
    "swimmersJSON.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Notice that Spark was able to determine infer the schema (when reviewing the schema using .printSchema).\n",
    " - But what if we want to programmatically specify the schema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programmatically Specifying the Schema\n",
    "In this case, let's specify the schema for a CSV text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(123, 'Katie', 19, 'brown'),\n",
       " (234, 'Michael', 22, 'green'),\n",
       " (345, 'Simone', 23, 'blue')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Generate our own CSV data \n",
    "#   This way we don't have to access the file system yet.\n",
    "stringCSVRDD = sc.parallelize([(123, 'Katie', 19, 'brown'), (234, 'Michael', 22, 'green'), (345, 'Simone', 23, 'blue')])\n",
    "stringCSVRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types\n",
    "schemaString = \"id name age eyeColor\"\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),    \n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", LongType(), True),\n",
    "    StructField(\"eyeColor\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply the schema to the RDD and Create DataFrame\n",
    "swimmers = spark.createDataFrame(stringCSVRDD, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates a temporary view using the DataFrame\n",
    "swimmers.createOrReplaceTempView(\"swimmers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema\n",
    "#   Notice that we have redefined id as Long (instead of String)\n",
    "swimmers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-1de4d92a13a6>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-1de4d92a13a6>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    -- Query the data\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%sql \n",
    "-- Query the data\n",
    "select * from swimmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------+\n",
      "| id|   name|age|eyeColor|\n",
      "+---+-------+---+--------+\n",
      "|123|  Katie| 19|   brown|\n",
      "|234|Michael| 22|   green|\n",
      "|345| Simone| 23|    blue|\n",
      "+---+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from swimmers\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [work]",
   "language": "python",
   "name": "Python [work]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
