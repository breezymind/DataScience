{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LearningPySpark_Ch03\n",
    "## Chapter 3: DataFrames\n",
    "This notebook contains sample code from Chapter 4 of <a href=\"https://render.githubusercontent.com/view/ipynb?commit=edcaf5ed42558fb08759f70a01201e00aa2d49f7&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f5061636b745075626c697368696e672f4c6561726e696e672d5079537061726b2f656463616635656434323535386662303837353966373061303132303165303061613264343966372f4368617074657230322f4c6561726e696e675079537061726b5f4368617074657230332e6970796e62&nwo=PacktPublishing%2FLearning-PySpark&path=Chapter02%2FLearningPySpark_Chapter03.ipynb&repository_id=83264169\">Learning PySpark</a> focusing on PySpark and DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - A DataFrame is an immutable distributed collection of data that is organized into named columns analogous to a table in a relation database\n",
    " - Python Pandas DataFrame or R DataFrame, a Spark DataFrame is a similar concept in that it allows users to easily work with structured data \n",
    "  - some differenceses as well so please temper your expectations.\n",
    " - specifically, the Catalyst Optimizer - to significantly improve the performance of Spark queries. \n",
    "  - In earlier APIs of Spark(that's RDDs), executing queries in python could be significantly slower due to communication overhead between the Java JVM and Py4J.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python to RDD communications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - PySpark program is executed using RDDs, there's potentially large overhead to execute the job\n",
    "  - Whenever PySpark execute some code with RDDs, PySpark Driver, the $Spark Context$ uses $Py4j$ 'JVM' using the JavaSparkContext. \n",
    " - Any RDD trasnformations are mapped to PythonRDD objects in java\n",
    " - these tasks are pushed out to the Spark Worker(s), PythonRDD objects launch Python subprocesses using pipes to send both code and data to be processed within Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img1](img/1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catalyst Optimizer refresh\n",
    " - one of the primary reasons the Spark SQL engine is so fast is because of the $Catalyst$ $Optimizer$.\n",
    "![img1](img/1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames\n",
    "### Generate your own DataFrame\n",
    "- Instead of accessing the file system, let's create a DataFrame by generating the data. \n",
    "- In this case, we'll first create the stringRDD RDD and then convert it into a DataFrame when we're reading stringJSONRDD using spark.read.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:475"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate our own JSON data \n",
    "#   This way we don't have to access the file system yet.\n",
    "stringJSONRDD = sc.parallelize((\"\"\" \n",
    "  { \"id\": \"123\",\n",
    "    \"name\": \"Katie\",\n",
    "    \"age\": 19,\n",
    "    \"eyeColor\": \"brown\"\n",
    "  }\"\"\",\n",
    "   \"\"\"{\n",
    "    \"id\": \"234\",\n",
    "    \"name\": \"Michael\",\n",
    "    \"age\": 22,\n",
    "    \"eyeColor\": \"green\"\n",
    "  }\"\"\", \n",
    "  \"\"\"{\n",
    "    \"id\": \"345\",\n",
    "    \"name\": \"Simone\",\n",
    "    \"age\": 23,\n",
    "    \"eyeColor\": \"blue\"\n",
    "  }\"\"\")\n",
    ")\n",
    "stringJSONRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\n  { \"id\": \"123\",\\n    \"name\": \"Katie\",\\n    \"age\": 19,\\n    \"eyeColor\": \"brown\"\\n  }',\n",
       " '{\\n    \"id\": \"234\",\\n    \"name\": \"Michael\",\\n    \"age\": 22,\\n    \"eyeColor\": \"green\"\\n  }',\n",
       " '{\\n    \"id\": \"345\",\\n    \"name\": \"Simone\",\\n    \"age\": 23,\\n    \"eyeColor\": \"blue\"\\n  }']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringJSONRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n",
       " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
       " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "swimmersJSON = spark.read.json(stringJSONRDD)\n",
    "swimmersJSON.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create temporary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create temporary table\n",
    "swimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple DataFrame Queries\n",
    "#### DataFrame API Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 19|   brown|123|  Katie|\n",
      "| 22|   green|234|Michael|\n",
      "| 23|    blue|345| Simone|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame API\n",
    "swimmersJSON.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL Query\n",
    " - $.collect()$ methond, which returns all the records as a list of Row objects\n",
    " - $collect()$ or $show()$ method for both DataFrames and SQL queries\n",
    " - $.collect()$ is for a small DataFrame, shince it will return all of the rows in the DataFrame and move them back from the executors to the driver\n",
    "  - $take(<n>)$ or $show(<n>)$, allow you to limit the number of rows returned by specifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n",
       " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
       " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL Query\n",
    "spark.sql(\"select * from swimmersJSON\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if you are using Databricks, i can use %sql command\n",
    "#%sql \n",
    "#-- Query Data\n",
    "#select * from swimmersJSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interoperating with RDDs\n",
    "There are two different methods for converting existing RDDs to DataFrame(or Datasets)\n",
    " - inferring the schema using reflection, or programmatically specifying the schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferring the Schema Using Reflection\n",
    "Note that Apache Spark is inferring the schema using reflection; <br>\n",
    "i.e. it automaticlaly determines the schema of the data based on reviewing the JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema\n",
    "swimmersJSON.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### but What if we don't know or that's not the same type ?\n",
    " - the id is actually a long instead of a string?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Notice that Spark was able to determine infer the schema (when reviewing the schema using .printSchema).\n",
    " - But what if we want to programmatically specify the schema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programmatically Specifying the Schema\n",
    "In this case, let's specify the schema for a CSV text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(123, 'Katie', 19, 'brown'),\n",
       " (234, 'Michael', 22, 'green'),\n",
       " (345, 'Simone', 23, 'blue')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Generate our own CSV data \n",
    "#   This way we don't have to access the file system yet.\n",
    "stringCSVRDD = sc.parallelize([(123, 'Katie', 19, 'brown'), (234, 'Michael', 22, 'green'), (345, 'Simone', 23, 'blue')])\n",
    "stringCSVRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User define Structure and Variable's Type\n",
    " - StructField class\n",
    "  - name = The name of this field\n",
    "  - dataType = The data type of this field\n",
    "  - nullable = Indicates whether values of this field can be null \n",
    " - Almost Same things like RDBMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types\n",
    "schemaString = \"id name age eyeColor\"\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),    \n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", LongType(), True),\n",
    "    StructField(\"eyeColor\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply the schema to the RDD and Create DataFrame\n",
    "swimmers = spark.createDataFrame(stringCSVRDD, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates a temporary view using the DataFrame\n",
    "swimmers.createOrReplaceTempView(\"swimmers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema\n",
    "#   Notice that we have redefined id as Long (instead of String)\n",
    "swimmers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%sql \n",
    "#-- Query the data\n",
    "#select * from swimmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------+\n",
      "| id|   name|age|eyeColor|\n",
      "+---+-------+---+--------+\n",
      "|123|  Katie| 19|   brown|\n",
      "|234|Michael| 22|   green|\n",
      "|345| Simone| 23|    blue|\n",
      "+---+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from swimmers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from above, we can programmatically apply the schema instead of allowing the Spark engine to infer the schema via reflection.\n",
    "\n",
    "Additional Resources include:\n",
    " - <a href=\"https://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html\">PySpark API Reference</a>\n",
    " - <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema\">Spark SQL, DataFrames, and Datasets Guide </a>: This is in reference to Programmatically Specifying the Schema using a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|| SparkSession\n",
    "\n",
    "Notice that we're no longer using sqlContext.read... but instead spark.read.... This is because as part of Spark 2.0, HiveContext,  SQLContext, StreamingContext, SparkContext have been merged together into the Spark Session spark.\n",
    " - Entry point for reading data\n",
    " - Working with metadata\n",
    " - Configuration\n",
    " - Cluster resource management\n",
    "\n",
    "For more information, please refer to How to use <a href=\"http://bit.ly/2br0Fr1\">SparkSession</a> in Apache Spark 2.0 (http://bit.ly/2br0Fr1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying with the DataFrame API\n",
    " - can start off by using collect(), show(), or take() to view the data within DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swimmers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(swimmers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running filter statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query id and age for swimmers with age =22 via Dataframe API\n",
    "swimmers.select(\"id\",\"age\").filter(\"age=22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.select(swimmers.id, swimmers.age).filter(swimmers.age==22).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|eyeColor|\n",
      "+------+--------+\n",
      "| Katie|   brown|\n",
      "|Simone|    blue|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Query name and eye color for swimmers with eye color starting with the letter 'b'\n",
    "swimmers.select(\"name\",\"eyeColor\").filter(\"eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying with SQL (SQL engine)\n",
    " - Recall that this DataFrame is accessible because we executed the $.createOrReplaceTempView$ method for $swimmers$\n",
    "\n",
    "With DataFrames, you can start writing your queries using Spark SQL - a SQL dialect that is compatible with the Hive Query Language (or HiveQL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------+\n",
      "| id|   name|age|eyeColor|\n",
      "+---+-------+---+--------+\n",
      "|123|  Katie| 19|   brown|\n",
      "|234|Michael| 22|   green|\n",
      "|345| Simone| 23|    blue|\n",
      "+---+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL문 실행 / 데이터 출력 \n",
    "spark.sql(\"select * from swimmers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Number of Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from swimmers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running filter statements using the where Clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query id and age for swimmers with age = 22 in SQL \n",
    "spark.sql(\"select id, age from swimmers where age =22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|eyeColor|\n",
      "+------+--------+\n",
      "| Katie|   brown|\n",
      "|Simone|    blue|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Query name and eye color for swimmers with eye color starting with the letter 'b'\n",
    "spark.sql(\"select name, eyeColor from swimmers where eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame scenario – on-time flight performance (DataSets)\n",
    " - Airline On-Time Performance and Causes of Flight Delays: On-Time Data (http://bit.ly/2ccJPPM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set file Path\n",
    "flightPerfFilePath = \"./data/flight/departuredelays.csv\"\n",
    "airportsFilePath = \"./data/flight/airport-codes-na.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain Airports dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "airports = spark.read.csv(airportsFilePath,header=True,inferSchema=True, sep='\\t')\n",
    "airports.createOrReplaceTempView(\"airports\") # make tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(City='Abbotsford', State='BC', Country='Canada', IATA='YXX'),\n",
       " Row(City='Aberdeen', State='SD', Country='USA', IATA='ABR'),\n",
       " Row(City='Abilene', State='TX', Country='USA', IATA='ABI'),\n",
       " Row(City='Akron', State='OH', Country='USA', IATA='CAK'),\n",
       " Row(City='Alamosa', State='CO', Country='USA', IATA='ALS')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain Departure Delays dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flightPerf = spark.read.csv(flightPerfFilePath, header=True)\n",
    "flightPerf.createOrReplaceTempView(\"FlightPerformance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date='01011245', delay='6', distance='602', origin='ABE', destination='ATL'),\n",
       " Row(date='01020600', delay='-8', distance='369', origin='ABE', destination='DTW'),\n",
       " Row(date='01021245', delay='-2', distance='602', origin='ABE', destination='ATL'),\n",
       " Row(date='01020605', delay='-4', distance='602', origin='ABE', destination='ATL'),\n",
       " Row(date='01031245', delay='-4', distance='602', origin='ABE', destination='ATL')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightPerf.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache the Departure Delays dataset\n",
    " - finally, we cachethe flight dataset so subsequent queries will be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightPerf.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining flight performance and airports\n",
    " - One of the most common tasks with Dataframes / SQL is to join two different datasets with identified columns \n",
    "  - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Sum of Flight Delays by City and Origin Code\n",
    " - the total delays by city and origin code for the state of Washington"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|   City|origin|  Delays|\n",
      "+-------+------+--------+\n",
      "|Seattle|   SEA|159086.0|\n",
      "|Spokane|   GEG| 12404.0|\n",
      "|  Pasco|   PSC|   949.0|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select a.City, f.origin, sum(f.delay) as Delays \n",
    "from FlightPerformance f join airports a \n",
    "on a.IATA = f.origin where a.State = 'WA'\n",
    "group by a.City,f.origin \n",
    "order by Delays desc\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Using notebooks (such as Databricks, iPython, Jupyter, and Apache Zeppelin), you can more easily execute and visualize your queries\n",
    "![img2](img/2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing our flight-performance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n %sql\\n-- Query Sum of Flight Delays by State (for the US)\\nselect a.State, sum(f.delay) as Delays\\nfrom FlightPerformance f\\njoin airports a\\non a.IATA = f.origin\\nwhere a.Country = 'USA'\\ngroup by a.State\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " %sql\n",
    "-- Query Sum of Flight Delays by State (for the US)\n",
    "select a.State, sum(f.delay) as Delays\n",
    "from FlightPerformance f\n",
    "join airports a\n",
    "on a.IATA = f.origin\n",
    "where a.Country = 'USA'\n",
    "group by a.State\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img3](img/3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Dataset API\n",
    "![img4](img/4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    " - One of the main reasons Python is initially slower within Spark is due to the communication layer between Python sub-processes and the JVM.\n",
    " - Spark DataFrames has many performance enhancements through the Catalyst Optimizer and Project Tungsten which we have reviewed in this chapter.\n",
    " - we also reviewed how to work with Spark DataFrames and worked on an on-time flight performance scenario using DataFrames"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [work]",
   "language": "python",
   "name": "Python [work]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
