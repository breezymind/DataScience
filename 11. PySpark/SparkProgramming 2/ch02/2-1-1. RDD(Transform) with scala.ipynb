{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Transform with Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 SparkContext 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:\n",
       "org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)\n",
       "org.apache.toree.kernel.api.Kernel.createSparkContext(Kernel.scala:349)\n",
       "org.apache.toree.kernel.api.Kernel.createSparkContext(Kernel.scala:368)\n",
       "org.apache.toree.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:103)\n",
       "org.apache.toree.Main$$anon$1.initializeSparkContext(Main.scala:35)\n",
       "org.apache.toree.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:88)\n",
       "org.apache.toree.Main$$anon$1.initializeComponents(Main.scala:35)\n",
       "org.apache.toree.boot.KernelBootstrap.initialize(KernelBootstrap.scala:101)\n",
       "org.apache.toree.Main$.delayedEndpoint$org$apache$toree$Main$1(Main.scala:40)\n",
       "org.apache.toree.Main$delayedInit$body.apply(Main.scala:24)\n",
       "scala.Function0$class.apply$mcV$sp(Function0.scala:34)\n",
       "scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)\n",
       "scala.App$$anonfun$main$1.apply(App.scala:76)\n",
       "scala.App$$anonfun$main$1.apply(App.scala:76)\n",
       "scala.collection.immutable.List.foreach(List.scala:381)\n",
       "scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)\n",
       "scala.App$class.main(App.scala:76)\n",
       "org.apache.toree.Main$.main(Main.scala:24)\n",
       "org.apache.toree.Main.main(Main.scala)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "StackTrace: org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)\n",
       "org.apache.toree.kernel.api.Kernel.createSparkContext(Kernel.scala:349)\n",
       "org.apache.toree.kernel.api.Kernel.createSparkContext(Kernel.scala:368)\n",
       "org.apache.toree.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:103)\n",
       "org.apache.toree.Main$$anon$1.initializeSparkContext(Main.scala:35)\n",
       "org.apache.toree.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:88)\n",
       "org.apache.toree.Main$$anon$1.initializeComponents(Main.scala:35)\n",
       "org.apache.toree.boot.KernelBootstrap.initialize(KernelBootstrap.scala:101)\n",
       "org.apache.toree.Main$.delayedEndpoint$org$apache$toree$Main$1(Main.scala:40)\n",
       "org.apache.toree.Main$delayedInit$body.apply(Main.scala:24)\n",
       "scala.Function0$class.apply$mcV$sp(Function0.scala:34)\n",
       "scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)\n",
       "scala.App$$anonfun$main$1.apply(App.scala:76)\n",
       "scala.App$$anonfun$main$1.apply(App.scala:76)\n",
       "scala.collection.immutable.List.foreach(List.scala:381)\n",
       "scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)\n",
       "scala.App$class.main(App.scala:76)\n",
       "org.apache.toree.Main$.main(Main.scala:24)\n",
       "org.apache.toree.Main.main(Main.scala)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2472)\n",
       "  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2468)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2468)\n",
       "  at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2557)\n",
       "  at org.apache.spark.SparkContext.<init>(SparkContext.scala:85)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.{SparkContext, SparkConf}\n",
    "\n",
    "val conf = new SparkConf().setMaster(\"local[*]\").setAppName(\"RDDCreateSample\")\n",
    "val sc = new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rdd = sc.parallelize(List(\"a\",\"b\",\"c\",\"d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(a, b, c, d)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n"
     ]
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 10)\n",
    "val result = rdd.collect\n",
    "println(result.mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4.2 Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 10)\n",
    "val result = rdd.count\n",
    "println(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.1 Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2, 3, 4, 5, 6, 7, 8, 9, 10, 11\n"
     ]
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 10)\n",
    "val result = rdd.map(_ + 1)\n",
    "println(result.collect.mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.2 flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val fruits = List(\"apple, orange\",\"grape, apple, mango\", \"blueberry,tomato, orange\")\n",
    "val rdd1 = sc.parallelize(fruits)\n",
    "val rdd2 = rdd1.map(_.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{apple, orange},{grape, apple, mango},{blueberry,tomato, orange}}\n"
     ]
    }
   ],
   "source": [
    "println(rdd2.collect().map(_.mkString(\"{\",\",\",\"}\")).mkString(\"{\",\",\",\"}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple,  orange, grape,  apple,  mango, blueberry, tomato,  orange"
     ]
    }
   ],
   "source": [
    "val fruits = List(\"apple, orange\",\"grape, apple, mango\", \"blueberry,tomato, orange\")\n",
    "val rdd1 = sc.parallelize(fruits)\n",
    "val rdd2 = rdd1.flatMap(_.split(\",\"))\n",
    "print(rdd2.collect.mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apple 단어를 포함한 것들만 하고 싶다면 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:25: error: type mismatch;\n",
       " found   : Unit\n",
       " required: TraversableOnce[?]\n",
       "           if (log.contains(\"apple\")){\n",
       "                                     ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fruits = List(\"apple, orange\",\"grape, apple, mango\", \"blueberry,tomato, orange\")\n",
    "val rdd1 = sc.parallelize(fruits)\n",
    "val rdd2 = rdd1.flatMap(log => {\n",
    "    if (log.contains(\"apple\")){\n",
    " \n",
    "    }else{\n",
    "        None\n",
    "    }\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.3 mapPartitions\n",
    " - map(), flatMap()의 경우 각 요소를 하나씩 처리 \n",
    " - mapPartitions는 Parition별로 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,3,4,5,6,7,8,9,10,11\n"
     ]
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(1 to 10, 3)\n",
    "val rdd2 = rdd1.mapPartitions(numbers => {\n",
    "  print(\"DB 연결\") // Jupyter notebook은 나오지 않는다. \n",
    "  numbers.map{number => number + 1}\n",
    "})\n",
    "\n",
    "println(rdd2.collect.mkString(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.1.5.4 mapPartitionsWIthIndex\n",
    " - 인자로 받은 함수를 파티션 단위로 적용하고 그 결과값으로 구성된 새로운 RDD를 Return.\n",
    " - 해당 파티션의 인덱스 정보도 함께 전달. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd1 = sc.parallelize(1 to 10, 3)\n",
    "val rdd2 = rdd1.mapPartitionsWithIndex((idx, numbers) => {\n",
    "    numbers.flatMap{\n",
    "        case number if idx == 1 => Option(number + 1)\n",
    "        case _                   => None\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5, 6, 7\n"
     ]
    }
   ],
   "source": [
    "println(rdd2.collect.mkString(\", \")) // 첫번쨰 파티션에서만 데이터 추출."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.5 mapValues \n",
    " - Input : Pair RDD\n",
    " - RDD 요소가 Key, Value 값의 쌍을 이루고있을 경우 페어 RDD 용어를 사용. \n",
    " - 키를 기준으로 해서 작은 그룹들을 만들고 해당 그룹들에 속한 값을 대상으로 합계나 평균을 구하는 등의 연산을 수행하는 경우가 많다. 이럴때 많이 사용한다. \n",
    " - Key에 해당하는 부분은 그대로 두고 값에만 map() 연산을 적용한 것과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a,2)\t(b,2)\t(c,2)\n"
     ]
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"a\",\"b\",\"c\")).map((_,1))\n",
    "val result = rdd.mapValues(_+1)\n",
    "println(result.collect.mkString(\"\\t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.6 flatMapValues \n",
    " - Input : Pair RDD\n",
    " - 페어 RDD에 사용하며 map 대신에 flatMap() 함수를 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List((1,a,b), (2,a,c), (1,d,e))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Seq((1,\"a,b\"),(2, \"a,c\"),(1, \"d,e\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,a),(1,b),(2,a),(2,c),(1,d),(1,e)\n"
     ]
    }
   ],
   "source": [
    "val rdd = sc.parallelize(Seq((1,\"a,b\"),(2, \"a,c\"),(1, \"d,e\")))\n",
    "val result = rdd.flatMapValues(_.split(\",\")).collect.mkString(\",\")\n",
    "println(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [그룹과 관련된 연산]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.7 zip\n",
    " - zip() 연산은 두 개의 서로 다른 RDD를 각요소의 인덱스에 따라 하나의 (Key, Value) 쌍으로 묶어줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a,1),(b,2),(c,3)\n"
     ]
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(List(\"a\",\"b\",\"c\"))\n",
    "val rdd2 = sc.parallelize(List(1,2,3))\n",
    "val result = rdd1.zip(rdd2)\n",
    "println(result.collect.mkString(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.8 zipPartitions \n",
    " - zip 연산을 Partition 별로 적용하고 특정 함수를 적용하여 그 결과로 구성된 새로운 RDD를 생성. \n",
    " - zip : Parition 수, Elements 수 동일\n",
    " - zipPartition : Paritions 수 만 동일해도 상관없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1, b2, c3\n"
     ]
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(List(\"a\",\"b\",\"c\"), 3)\n",
    "val rdd2 = sc.parallelize(List(1,2,3), 3)\n",
    "val result = rdd1.zipPartitions(rdd2){\n",
    "    (it1, it2) => for{\n",
    "        v1 <- it1;\n",
    "        v2 <- it2\n",
    "    } yield v1 + v2\n",
    "} // (a, 1), (b, 2), (c, 3) 묶인 RDD를 각 값을 + (String합) 하는 Method 적용.\n",
    "\n",
    "println(result.collect.mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.9 Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "even, [2,4,6,8,10]\n",
      "odd, [1,3,5,7,9]\n"
     ]
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 10)\n",
    "val result = rdd.groupBy{\n",
    "    case i: Int if (i % 2 == 0) => \"even\"\n",
    "    case _                      => \"odd\"\n",
    "}\n",
    "result.collect.foreach(\n",
    "    v=> println(s\"${v._1}, [${v._2.mkString(\",\")}]\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.10 groupByKey\n",
    " - Input : Pair RDD\n",
    " - groupBy() 메서드가 요소의 키를 생성하는 작업과 그룹으로 분류하는 작업을 동시 수행\n",
    " - groupByKey()의 경우 이미 페어 RDD를 사용하여 키를 가진 요소들로 그룹을 만들고 새로운 RDD를 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a, [1]\n",
      "b, [1,1]\n",
      "c, [1,1]\n"
     ]
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"a\",\"b\",\"c\",\"b\",\"c\")).map((_,1))\n",
    "val result = rdd.groupByKey\n",
    "result.collect.foreach{\n",
    "    v => println(s\"${v._1}, [${v._2.mkString(\",\")}]\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.11 cogroup\n",
    " - Input : Pair RDD\n",
    " - 여러 RDD에서 같은 키를 갖는 값 요소를 찾아서 키와 그 키에 속하는 요소의 시퀀스로 구성된 튜플 생성 => RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(k1, [v1,v3], [v4])\n",
      "(k2, [v2], [])\n"
     ]
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(List((\"k1\",\"v1\"),(\"k2\",\"v2\"),(\"k1\",\"v3\")))\n",
    "val rdd2 = sc.parallelize(List((\"k1\",\"v4\")))\n",
    "val result = rdd1.cogroup(rdd2)\n",
    "result.collect.foreach{\n",
    "    case (k, (v_1, v_2)) => {\n",
    "        println(s\"($k, [${v_1.mkString(\",\")}], [${v_2.mkString(\",\")}])\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((k1,(CompactBuffer(v1, v3),CompactBuffer(v4))), (k2,(CompactBuffer(v2),CompactBuffer())))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### [집합과 관련된 연산들]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.12 distinct\n",
    " - 중복제거 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,3\n"
     ]
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(1,2,3,1,2,3,1,2,3))\n",
    "val result = rdd.distinct()\n",
    "println(result.collect.mkString(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.13 cartesian\n",
    " - 카테시안곲을 구하고 그결과를 RDD로 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,a), (1,b), (1,c), (2,a), (2,b), (2,c), (3,a), (3,b), (3,c)"
     ]
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(List(1,2,3))\n",
    "val rdd2 = sc.parallelize(List(\"a\", \"b\", \"c\"))\n",
    "val result = rdd1.cartesian(rdd2)\n",
    "print(result.collect.mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.14 Subtrack \n",
    " - 차집합. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d, e\n"
     ]
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(List(\"a\", \"b\", \"c\", \"d\", \"e\"))\n",
    "val rdd2 = sc.parallelize(List(\"a\", \"b\", \"c\"))\n",
    "val result = rdd1.subtract(rdd2)\n",
    "println(result.collect.mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.15 Union \n",
    " - 합집합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a, b, c, d, e, a, b, c\n"
     ]
    }
   ],
   "source": [
    "val result = rdd1.union(rdd2)\n",
    "println(result.collect.mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.16 Intersection\n",
    " - 교집합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a, b, c\n"
     ]
    }
   ],
   "source": [
    "val result = rdd1.intersection(rdd2)\n",
    "println(result.collect.mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.17 Join \n",
    " - Input : Pair RDD\n",
    " - 서로 같은 키를 가지고 있는 요소를 모아 그룹을 생성, 이후 새로운 RDD 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b,(1,2))\n",
      "(c,(1,2))\n"
     ]
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(List(\"a\", \"b\", \"c\", \"d\", \"e\")).map((_,1))\n",
    "val rdd2 = sc.parallelize(List(\"b\", \"c\")).map((_,2))\n",
    "val result = rdd1.join(rdd2)\n",
    "println(result.collect.mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
