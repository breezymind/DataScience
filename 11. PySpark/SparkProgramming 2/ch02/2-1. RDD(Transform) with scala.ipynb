{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Transform with Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 SparkContext 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:\n",
       "org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)\n",
       "org.apache.toree.kernel.api.Kernel.createSparkContext(Kernel.scala:349)\n",
       "org.apache.toree.kernel.api.Kernel.createSparkContext(Kernel.scala:368)\n",
       "org.apache.toree.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:103)\n",
       "org.apache.toree.Main$$anon$1.initializeSparkContext(Main.scala:35)\n",
       "org.apache.toree.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:88)\n",
       "org.apache.toree.Main$$anon$1.initializeComponents(Main.scala:35)\n",
       "org.apache.toree.boot.KernelBootstrap.initialize(KernelBootstrap.scala:101)\n",
       "org.apache.toree.Main$.delayedEndpoint$org$apache$toree$Main$1(Main.scala:40)\n",
       "org.apache.toree.Main$delayedInit$body.apply(Main.scala:24)\n",
       "scala.Function0$class.apply$mcV$sp(Function0.scala:34)\n",
       "scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)\n",
       "scala.App$$anonfun$main$1.apply(App.scala:76)\n",
       "scala.App$$anonfun$main$1.apply(App.scala:76)\n",
       "scala.collection.immutable.List.foreach(List.scala:381)\n",
       "scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)\n",
       "scala.App$class.main(App.scala:76)\n",
       "org.apache.toree.Main$.main(Main.scala:24)\n",
       "org.apache.toree.Main.main(Main.scala)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "StackTrace: org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)\n",
       "org.apache.toree.kernel.api.Kernel.createSparkContext(Kernel.scala:349)\n",
       "org.apache.toree.kernel.api.Kernel.createSparkContext(Kernel.scala:368)\n",
       "org.apache.toree.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:103)\n",
       "org.apache.toree.Main$$anon$1.initializeSparkContext(Main.scala:35)\n",
       "org.apache.toree.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:88)\n",
       "org.apache.toree.Main$$anon$1.initializeComponents(Main.scala:35)\n",
       "org.apache.toree.boot.KernelBootstrap.initialize(KernelBootstrap.scala:101)\n",
       "org.apache.toree.Main$.delayedEndpoint$org$apache$toree$Main$1(Main.scala:40)\n",
       "org.apache.toree.Main$delayedInit$body.apply(Main.scala:24)\n",
       "scala.Function0$class.apply$mcV$sp(Function0.scala:34)\n",
       "scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)\n",
       "scala.App$$anonfun$main$1.apply(App.scala:76)\n",
       "scala.App$$anonfun$main$1.apply(App.scala:76)\n",
       "scala.collection.immutable.List.foreach(List.scala:381)\n",
       "scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)\n",
       "scala.App$class.main(App.scala:76)\n",
       "org.apache.toree.Main$.main(Main.scala:24)\n",
       "org.apache.toree.Main.main(Main.scala)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2472)\n",
       "  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2468)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2468)\n",
       "  at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2557)\n",
       "  at org.apache.spark.SparkContext.<init>(SparkContext.scala:85)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.{SparkContext, SparkConf}\n",
    "\n",
    "val conf = new SparkConf().setMaster(\"local[*]\").setAppName(\"RDDCreateSample\")\n",
    "val sc = new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rdd = sc.parallelize(List(\"a\",\"b\",\"c\",\"d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(a, b, c, d)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n"
     ]
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 10)\n",
    "val result = rdd.collect\n",
    "println(result.mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4.2 Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 10)\n",
    "val result = rdd.count\n",
    "println(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.1 Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2, 3, 4, 5, 6, 7, 8, 9, 10, 11\n"
     ]
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 10)\n",
    "val result = rdd.map(_ + 1)\n",
    "println(result.collect.mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.2 flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val fruits = List(\"apple, orange\",\"grape, apple, mango\", \"blueberry,tomato, orange\")\n",
    "val rdd1 = sc.parallelize(fruits)\n",
    "val rdd2 = rdd1.map(_.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{apple, orange},{grape, apple, mango},{blueberry,tomato, orange}}\n"
     ]
    }
   ],
   "source": [
    "println(rdd2.collect().map(_.mkString(\"{\",\",\",\"}\")).mkString(\"{\",\",\",\"}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple,  orange, grape,  apple,  mango, blueberry, tomato,  orange"
     ]
    }
   ],
   "source": [
    "val fruits = List(\"apple, orange\",\"grape, apple, mango\", \"blueberry,tomato, orange\")\n",
    "val rdd1 = sc.parallelize(fruits)\n",
    "val rdd2 = rdd1.flatMap(_.split(\",\"))\n",
    "print(rdd2.collect.mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apple 단어를 포함한 것들만 하고 싶다면 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:25: error: type mismatch;\n",
       " found   : Unit\n",
       " required: TraversableOnce[?]\n",
       "           if (log.contains(\"apple\")){\n",
       "                                     ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fruits = List(\"apple, orange\",\"grape, apple, mango\", \"blueberry,tomato, orange\")\n",
    "val rdd1 = sc.parallelize(fruits)\n",
    "val rdd2 = rdd1.flatMap(log => {\n",
    "    if (log.contains(\"apple\")){\n",
    " \n",
    "    }else{\n",
    "        None\n",
    "    }\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.3 mapPartitions\n",
    " - map(), flatMap()의 경우 각 요소를 하나씩 처리 \n",
    " - mapPartitions는 Parition별로 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,3,4,5,6,7,8,9,10,11\n"
     ]
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(1 to 10, 3)\n",
    "val rdd2 = rdd1.mapPartitions(numbers => {\n",
    "  print(\"DB 연결\")\n",
    "  numbers.map{number => number + 1}\n",
    "})\n",
    "\n",
    "println(rdd2.collect.mkString(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
