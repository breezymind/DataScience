install.packages("base64enc")
install.packages(c("RCurl","twitteR","ROAuth"))
library("base64enc")
library("RCurl")
library("twitteR")
library("ROAuth")
reqURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
consumerKey <- "J6Y4NEAl5mubjrHQ28kFbWAsw"
consumerSecret <- "qbnzZOVPcQjAU0MbKbFybEw5jtcrZTlhs1djEi0Xaix9Ev1gxH"
accesstoken <- "896621426-KC4s40IVAlYWhrL61N6RGkxlUMfkirUjGFdvZaju"
accesstokensecret <- "JQ9ZT8j2VY2wKZiehy2IKU5toSkcnfhHevpx0x0Hq6Jn0"
options (RCurlOptions = list(cainfo = system.file("CurlSSL","cacert.pem",package = "RCurl")))
download.file(url="https://curl.haxx.se/ca/carcert.pem",destfile="cacert.pem")
setup_twitter_oauth(consumerKey,consumerSecret,accesstoken,accesstokensecret)
keyword <- enc2utf8("박유천")
lotteworld <-  searchTwitter(keyword,n=10)
lotteworld <- matrix(unlist(lotteworld),ncol = 17,byrow = T)
lotteworld
lotteworld <-  searchTwitter(keyword,n=10)
lotteworld
SeoulJazzFest.tweets <- searchTwitter("@SeoulJazzFest", n = 100)
SeoulJazzFest.tweets
Jedong.tweets <- searchTwitter("@keumkangkyung", n = 100)
Jedong.tweets
JTBC.tweets <- searchTwitter("@jtbclove", n = 100)
JTBC.tweets
install.packages("KoNLP")
install.packages("wordcloud")
install.packages("plyr")
library(KoNLP)
library(wordcloud)
library(plyr)
keyword <- "#h3"
h3_twitter <- searchTwitter(keyword,since='2012-10-29',geocode='35.874,128.246,400km',n=100)
keyword <- "#쯔위"
h3_twitter <- searchTwitter(keyword,since='2012-10-29',geocode='35.874,128.246,400km',n=100)
h3_twitter.df <- twListToDF(h3_twitter)
h3_twitter.df
View(h3_twitter.df)
str(h3_twitter.df)
h3_twitter.df$text
h3_twitter.text <- h3_twitter.df$text
h3_twitter.text <- gsub("\n", "", h3_twitter.text)
h3_twitter.text <- gsub("\r", "", h3_twitter.text)
h3_twitter.text <- gsub("RT", "", h3_twitter.text)
h3_twitter.text <- gsub("H3", "", h3_twitter.text)
h3_twitter.text <-gsub("개발자","",h3_twitter.text)
h3_twitter.text <-gsub("컨퍼런스","",h3_twitter.text)
h3_twitter.text <-gsub("co","",h3_twitter.text)
h3_twitter.text <-gsub("세션","",h3_twitter.text)
h3_twitter.text <-gsub("h3","",h3_twitter.text)
h3_twitter.text <-gsub("2012","",h3_twitter.text)
h3_twitter.text <-gsub("http","",h3_twitter.text)
h3_twitter_nouns <- Map(extractNoun, h3_twitter.text)
head(h3_twitter_nouns)
?Map
h3_twitter_word <- unlist(h3_twitter_nouns, use.name=F)
h3_twitter_word <- h3_twitter_word[-which(h3_twitter_word %in% stopwords("english"))]
h3_twitter <- searchTwitter(keyword,since='2015-10-29',geocode='35.874,128.246,400km',n=100)
h3_twitter <- searchTwitter(keyword,since='2015-10-29',until='2016-06-21' lang='ko',n=100)
h3_twitter <- searchTwitter(keyword,since='2015-10-29',until='2016-06-21' lang="ko",n=100)
h3_twitter <- searchTwitter(keyword,since='2015-10-29',until='2016-06-21', lang="ko",n=100)
twitter <- searchTwitter(keyword,since='2015-10-29',until='2016-06-21', lang="ko",n=100)
twitter.df <- twListToDF(twitter)  #data.frame 형태로 해당 트위터의 정보를 추출한다.
View(twitter.df)
twitter.text <- twitter.df$text
twitter.text <- gsub("\n", "", twitter.text)
twitter.text <- gsub("\r", "", twitter.text)
twitter.text <- gsub("RT", "", twitter.text)
twitter.text <- gsub("http", "", twitter.text)
result_nouns <- Map(extractNoun, twitter.text)
head(result_nouns)
result_wordsvec  <- unlist(twitter.text, use.name=F)
head(result_wordsvec)
result_wordsvec  <- result_wordsvec [-which(result_wordsvec  %in% stopwords("english"))]
library(tm)
install.packages("tm")
ininstall.packages("Unicode)
ininstall.packages("Unicode")
ininstall.packages("Unicode")
install.packages("Unicode")
library(tm)
library(Unicode)
result_wordsvec  <- result_wordsvec [-which(result_wordsvec  %in% stopwords("english"))]
result_wordsvec
result_wordsvec  <- unlist(twitter.text, use.name=F)
result_wordsvec  <- result_wordsvec [-which(result_wordsvec  %in% stopwords("english"))]
head(result_wordsvec)
options (RCurlOptions = list(cainfo = system.file("CurlSSL","cacert.pem",package = "RCurl")))
download.file(url="https://curl.haxx.se/ca/carcert.pem",destfile="cacert.pem")
cred <- OAuthFactory$new(consumerKey=consumerKey,
consumerSecret=consumerSecret,
requestURL="https://api.twitter.com/oauth/request_token",
accessURL="https://api.twitter.com/oauth/access_token",
authURL="https://api.twitter.com/oauth/authorize")
cred$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
setup_twitter_oauth(consumerKey,consumerSecret,accesstoken,accesstokensecret)
keyword <- enc2utf8("박유천")
lotteworld <-  searchTwitter(keyword,n=10)
tmp <- unlist(lotteworld,use.names = TRUE)
head(lotteworld)
options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))
registerTwitterOAuth(cred)
keyword <- "날씨"
# 트위터에서 키워드로 검색
result <- searchTwitter(keyword, since="2013-12-01", until="2013-12-04", lang="ko",n=100)
result <- searchTwitter(keyword, since="2016-05-01", until="2016-6-21", lang="ko",n=100)
result.df <- twListToDF(result)
result.text <- result.df$text
result.text <- gsub(keyword, "", result.text)
result.text <- gsub("\n", "", result.text)
result.text <- gsub("\r", "", result.text)
result.text <- gsub("RT", "", result.text)
result.text <- gsub("http", "", result.text)
# 문자 분리
result_nouns <- Map(extractNoun, result.text)
result_wordsvec <- unlist(result_nouns, use.name=F)
result_wordsvec
result_wordsvec <- result_wordsvec[-which(result_wordsvec %in% stopwords("english"))]
result_wordsvec
result_wordsvec <- gsub("[[:punct:]]","", result_wordsvec)
result_wordsvec
result_wordsvec <- Filter(function(x){nchar(x)>=2}, result_wordsvec)
result_wordsvec
result_wordcount <- table(result_wordsvec)
pal <- brewer.pal(12,"Paired")
wordcloud(names(result_wordcount), freq=result_wordcount, scale=c(5,0.5), min.freq=5, random.order=F, rot.per=.1, colors=pal, family="AppleGothic")
keyword <- "#쯔위"
twitter <- searchTwitter(keyword,since='2015-10-29',until='2016-06-21', lang="ko",n=100)
twitter.df <- twListToDF(twitter)  #data.frame 형태로 해당 트위터의 정보를 추출한다.
twitter.text <- twitter.df$text
twitter.text
twitter.text <- gsub("\n", "", twitter.text)
twitter.text <- gsub("\r", "", twitter.text)
twitter.text <- gsub("RT", "", twitter.text)
twitter.text <- gsub("http", "", twitter.text)
result_nouns <- Map(extractNoun, twitter.text)
result_wordsvec  <- unlist(result_nouns, use.name=F)
result_wordsvec  <- result_wordsvec [-which(result_wordsvec  %in% stopwords("english"))]
result_wordsvec
result_wordsvec  <- gsub("[[:punct:]]","", result_wordsvec )
result_wordsvec  <- Filter(function(x){nchar(x)>=2}, result_wordsvec )
twitter_count <- table(result_wordsvec)
twitter_count
twitter.text <- gsub("ㅠ", "", twitter.text)
twitter.text <- gsub("ㅋ", "", twitter.text)
result_nouns <- Map(extractNoun, twitter.text)
result_wordsvec  <- unlist(result_nouns, use.name=F)
result_wordsvec  <- result_wordsvec [-which(result_wordsvec  %in% stopwords("english"))]
result_wordsvec  <- gsub("[[:punct:]]","", result_wordsvec )
result_wordsvec  <- Filter(function(x){nchar(x)>=2}, result_wordsvec )
twitter_count <- table(result_wordsvec)
twitter_count
head(twitter_count)
str(twitter_count)
head(sort(twitter_count,decreasing=T),30)
str(twitter_count)
twitter_count <- as.data.frame.matrix(twitter_count)
twitter_count
head(sort(twitter_count,decreasing=T),30)
wordcloud(names(twitter_count),freq=twitter_count,scale=c(4,0.5),min.freq=1,
random.order=F,rot.per=.1,colors=pal,family="malgun")
tmp <- head(sort(twitter_count,decreasing=T),30)
wordcloud(names(tmp),freq=tmp,scale=c(4,0.5),min.freq=1,
random.order=F,rot.per=.1,colors=pal,family="malgun")
View(twitter.df)
twitter.df$retweetCount
twitter.df <- twitter.df[twitter.df$retweetCount<>0,]
twitter.df <- twitter.df[twitter.df$retweetCount!=0,]
twitter.df$retweetCount
dm <- dmFactory$new(text='foo', recipientSN='blah')
dm$getText()
dm <- dmFactory$new(json)
apple <- searchTwitter("@apple",n=100)
length(apple)
class(apple)
head(apple)
tweet.apple <- apple[[1]]
tweet.apple
tweet.apple$getId()
tweet.apple$favoriteCount
tweet.apple$isRetweet
tweet.apple$latitude
tweet.apple$getScreenName()
tweet.apple$text
apple.text <- lapply(apple$text, function(t) t$getText())
apple.text
apple.text <- lapply(apple, function(t) t$getText())
apple.text
apple.text <- laply(apple, function(t) t$getText())
apple.text
head(apple.text)
class(apple.text)
str(apple.text)
head(apple.text)
pos.word <- scan("positive-words.txt",what="character",comment.char = ";")
neg.word <- scan("negative-words.txt",what="character",comment.char = ";")
pos.word <- c(pos.word,"upgrade")
neg.word <- c(neg.word,"wait","waiting")
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
# we got a vector of sentences. plyr will handle a list or a vector as an "l" for us
# we want a simple array ("a") of scores back, so we use
# "l" + "a" + "ply" = "laply":
scores = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
apple.text <- apple.text[!Encoding(apple.text)=="UTF-8"]
apple.score <- score.sentiment(apple.text,pos.words,neg.word,.progress = 'text')
apple.score <- score.sentiment(apple.text,pos.word,neg.word,.progress = 'text')
hist(apple.score$score
)
apple <- searchTwitter("@apple",n=1000)
apple.text <- laply(apple, function(t) t$getText())
apple.text <- apple.text[!Encoding(apple.text)=="UTF-8"]
apple.score <- score.sentiment(apple.text,pos.word,neg.word,.progress = 'text')
hist(apple.score$score)
library("base64enc")
library("RCurl")
library("twitteR")
library("ROAuth")
library(KoNLP)
library(wordcloud)
library(plyr)
library(tm)
library(Unicode)
reqURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
# 트위터에서 받은 네개의 키 값을 변수에 할당.
consumerKey <- "J6Y4NEAl5mubjrHQ28kFbWAsw"
consumerSecret <- "qbnzZOVPcQjAU0MbKbFybEw5jtcrZTlhs1djEi0Xaix9Ev1gxH"
accesstoken <- "896621426-KC4s40IVAlYWhrL61N6RGkxlUMfkirUjGFdvZaju"
accesstokensecret <- "JQ9ZT8j2VY2wKZiehy2IKU5toSkcnfhHevpx0x0Hq6Jn0"
cred <- OAuthFactory$new(consumerKey=consumerKey,
consumerSecret=consumerSecret,
requestURL="https://api.twitter.com/oauth/request_token",
accessURL="https://api.twitter.com/oauth/access_token",
authURL="https://api.twitter.com/oauth/authorize")
cred$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))
registerTwitterOAuth(cred)
setup_twitter_oauth(consumerKey,consumerSecret,accesstoken,accesstokensecret)
apple <- searchTwitter("@BREXIT",n=1000)
length(apple)
head(apple)
apple.text <- laply(apple, function(t) t$getText())
head(apple.text)
pos.word <- scan("positive-words.txt",what="character",comment.char = ";")
neg.word <- scan("negative-words.txt",what="character",comment.char = ";")
pos.word <- c(pos.word,"upgrade")
neg.word <- c(neg.word,"wait","waiting")
pos.word <- c(pos.word,"freedom","Independence")
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
# we got a vector of sentences. plyr will handle a list or a vector as an "l" for us
# we want a simple array ("a") of scores back, so we use
# "l" + "a" + "ply" = "laply":
scores = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
apple.text <- apple.text[!Encoding(apple.text)=="UTF-8"]
apple.score <- score.sentiment(apple.text,pos.word,neg.word,.progress = 'text')
hist(apple.score$score)
apple.text
apple.score
head(apple.score)
View(apple.score)
View(pos.word)
neg.word <- c(neg.word,"wait","waiting","leave","withdraw","out","quit","broke","cancel","get out")
apple.text <- apple.text[!Encoding(apple.text)=="UTF-8"]
apple.score <- score.sentiment(apple.text,pos.word,neg.word,.progress = 'text')
hist(apple.score$score)
neg.word <- c(neg.word,"wait","waiting","leave","withdraw","out","quit","broke","cancel","get out","swallow")
apple <- searchTwitter("@BREXIT",n=10000)
