install.packages("nnet") # 자기가 만든 모델을 시각화 할 수 없다.
library(nnet); library(caret); library(ROCR)
cb <- read.delim("Hshopping.txt", stringsAsFactors=FALSE)
cb$반품여부 <- factor(cb$반품여부)	# 명목형 값 예측일 경우
set.seed(1)
inTrain <- createDataPartition(y=cb$반품여부, p=0.6, list=FALSE)
cb.train <- cb[inTrain,]
cb.test <- cb[-inTrain,]
source('D:/DataScience/KMU/Second Semester/DataMining/1105_Neural Network_2/p1105.R', encoding = 'UTF-8', echo=TRUE)
install.packages("nnet")
cb <- read.delim("Hshopping.txt", stringsAsFactors=FALSE)
cb$반품여부 <- factor(cb$반품여부)	# 명목형 값 예측일 경우
set.seed(1)
inTrain <- createDataPartition(y=cb$반품여부, p=0.6, list=FALSE)
cb.train <- cb[inTrain,]
cb.test <- cb[-inTrain,]
set.seed(123)
nn_model <- nnet(반품여부 ~ 성별+나이+구매금액+출연자, data=cb.train, size=3, maxit=1000)	# size: # of hidden nodes
library(nnet); library(caret); library(ROCR)
install.packages("nnet") # 자기가 만든 모델을 시각화 할 수 없다.
library(nnet); library(caret); library(ROCR)
cb <- read.delim("Hshopping.txt", stringsAsFactors=FALSE)
cb$반품여부 <- factor(cb$반품여부)	# 명목형 값 예측일 경우
set.seed(1)
inTrain <- createDataPartition(y=cb$반품여부, p=0.6, list=FALSE)
cb.train <- cb[inTrain,]
cb.test <- cb[-inTrain,]
set.seed(123)
nn_model <- nnet(반품여부 ~ 성별+나이+구매금액+출연자, data=cb.train, size=3, maxit=1000)	# size: # of hidden nodes
library(devtools)
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
plot.nnet(nn_model)
library(NeuralNetTools)
install.packages("NeuralNetTools")
library(NeuralNetTools)
garson(nn_model)
predict(nn_model, newdata=cb.test, type="class")
install.packages("nnet")   # 시각화에 부족
library(nnet); library(caret); library(ROCR)
cb <- read.delim("../1022_Decision Tree_2//Hshopping.txt", stringsAsFactors=FALSE)
cb$반품여부 <- factor(cb$반품여부)	# 명목형 값 예측일 경우
# C5.0은 명목형만 예측 가능
# Neural Network은 모두 가능하나, 추가 기능을 위해서는 바꿔두자!
set.seed(1)
inTrain <- createDataPartition(y=cb$반품여부, p=0.6, list=FALSE)
cb.train <- cb[inTrain,]
cb.test <- cb[-inTrain,]
nrow(cb.train)
nrow(cb)
set.seed(123)
nn_model <- nnet(반품여부 ~ 성별+나이+구매금액+출연자, data=cb.train, size=3, maxit=1000)	# size=hidden node수
nn_model
nn_model2 <- nnet(반품여부 ~ 성별+나이+구매금액+출연자, data=cb.train, size=3, maxit=1000, decay=0.0005)	# size=hidden node수
nn_model2
# decay 값 : learing rate를 조절할 수 있는 변수.
# i가 input값이다.
# b : bias (세타 값 : 시그모이드 함수의 임계)
# rang 옵션 : weight의 값들의 범위를 지정해 줄 수 있다.
# size: # of hidden nodes
#     늘리면 모델이 복잡해 진다 (학습모델 성능은 올라가나, Overfitting issue 발생)
# maxit: 300명을 1,000번 반복한다 (어느 정도 수렴되면 중간에 멈출 수도 있다)
summary(nn_model)
# 맨 앞줄의 b->h_i들의 값들이 시그모이드 임계 (세타)
# h1의 weight는 26.33
# 2번째 입력층에서 ~까지의 weight는...
# 선 12개 from 4개 변수 to 3개 노드 + 3개 노드 to 결과치 1개 노드 + bias 4개 = 19 weight
confusionMatrix(predict(nn_model, newdata=cb.test, type="class"), cb.test$반품여부)
confusionMatrix(predict(nn_model2, newdata=cb.test, type="class"), cb.test$반품여부)
library(devtools)
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
plot.nnet(nn_model)
plot.nnet(nn_model2)
# 선의 굵기에 따라서 중요도? 해당 output에 영향이 많이 가는 정도를 표현해준다.
# nnet을 돌릴때마다 달라지고 해당 traing data set에 따라서 달라진다.
library(NeuralNetTools)
garson(nn_model)   # by 변수 중요도 순서
# Nnet은 모델을 다 보더라도, weight를 계산하더라도, 이 변수의 의미를 보기에는 부족하다
# 모델 전체의 의미로만 참고용으로 봐야 한다
# whitebox model = 의사결정나무
# blackbox model = neural network
confusionMatrix(predict(nn_model, newdata=cb.test, type="class"), cb.test$반품여부)
# predict만 넣으면, 예측 확율이 나오고 class라고 넣었으니 실제 predict rate이 나온다
# 같은 predict여도, C5.0 or Neural에 따라 결과값이 달라진다
nn_pred <- ROCR::prediction(predict(nn_model,
newdata=cb.test, type="raw"), cb.test$반품여부)
# 각 패키지 각자에게 prediction이 다 다른 함수로 들어가 있다
# 그래서 ROCR 내에서의 prediction을 돌리라는 의미로 ROCR::prediction
# neural network의 실제값으로 뽑아라: type"raw"
par(mfrow=c(1,2))
nn_model.perf1 <- performance(nn_pred, "tpr", "fpr") # ROC-chart
nn_model.perf2 <- performance(nn_pred, "lift", "rpp") # Lift chart
plot(nn_model.perf1, colorize=TRUE); plot(nn_model.perf2, colorize=TRUE)
# decay: data 최적화를 시키려다 보니, weight가 너무 커지는 노드가 발생할 수 있음
#        다른 data로 적용 시, 정확도 이슈 발생
#        Error 함수 as E(w) : weight가 커지면 Error가 커진다
#        람다 as decay값 (커질수록 w가 커지는 것을 방지한다)
##### 2. Neural Network Analysis using neuralnet package
library(neuralnet)
cb <- read.delim("Hshopping.txt", stringsAsFactors=FALSE)
set.seed(1)
inTrain <- createDataPartition(y=cb$반품여부, p=0.6, list=FALSE)
cb.train <- cb[inTrain,]
cb.test <- cb[-inTrain,]
set.seed(123)
nn2_model <- neuralnet(반품여부 ~ 성별+나이+구매금액+출연자,
data=cb.train, hidden=3, threshold=0.01)
nn2_model2 <- neuralnet(반품여부 ~ 성별+나이+구매금액+출연자,
data=cb.train, hidden=c(2,3), threshold=0.01)
# error change가 0.01보다 작아지면 stop하라는 의미 (threshold)
# hidden=c(2,2) 첫번째 layer node2  두번째 layer node 2개
# hidden=c(2,3) 첫번째 layer node2  두번째 layer node 3개
# nnet 패키지보다 parameter가 많다 (threshold나 stepmax 둘 중 하나를 주로 사용한다,
#                                   굳이 둘다 쓸 필요가 없기 때문)
# linear.output:T = 출력층에는 굳이 log상용화 하지 않고 weight sum한다
#                             (sigmoid 함수 적용X)
plot(nn2_model)
plot(nn2_model2)
