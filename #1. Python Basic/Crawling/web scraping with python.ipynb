{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹 스크래핑(web scraping with python)\n",
    "    어떠한 웹페이지의 HTML 내용을 분석하여 원하는 값을 추출해내는 것을 스크래핑(scraping)이라고 합니다.\n",
    "    python의 기본패키지 및 BeautifulSoup을 이용해 daum.net의 검색결과 페이지를 스크래핑하는 예제 코드를 첨부합니다.\n",
    "\n",
    "    이 코드를 실행하려면 먼저 BeautifulSoup 패키지를 설치해야 합니다.\n",
    "    BeautifulSoup 패키지의 이름은 간단하게 'bs4' 입니다.\n",
    "\n",
    "    그래서 윈도우 커맨드 창에서 'easy_install bs4' 라고 실행하면 패키지를 설치할 수 있습니다.\n",
    "    그런데 easy_install 를 설치하지 않았다면 설치해야 합니다.\n",
    "    \n",
    "    \n",
    "    bs4 설치방법\n",
    "    1) easy_install을 설치한다 방법은 http://blog.colab.kr/11 참고.\n",
    "    2) easy_install 이 설치된 후에 윈도우 command 창에서 다음과 같이 실행한다.\n",
    "    $ easy_install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import urllib2\n",
    "import urllib\n",
    "import urlparse\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "USER_AGENT_CHROME = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1500.37 Safari/537.36\"\n",
    "USER_AGENT_FIREFOX = \"Mozilla/5.0 (X11; U; FreeBSD i386; en-US; rv:1.7.8) Gecko/20050609 Firefox/1.0.4\"\n",
    "AGENTS = [USER_AGENT_FIREFOX, USER_AGENT_CHROME]\n",
    "agent_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetch(web_url):\n",
    "  '''\n",
    "  주어진 웹페이지의 HTML 내용을 읽어옴\n",
    "  :param web_url: the url of a webpage\n",
    "  :return: HTML contents (문자열)\n",
    "  '''\n",
    "  global agent_cnt\n",
    "  agent_cnt += 1\n",
    "  try:\n",
    "    req = urllib.Request(web_url)\n",
    "    req.add_header(\"User-Agent\", AGENTS[agent_cnt % 2])\n",
    "    req.add_header(\"Connection\", \"Keep-Alive\")\n",
    "    #print req.headers\n",
    "    f = urllib.urlopen(req)\n",
    "    html = f.read()\n",
    "    #print len(html)\n",
    "    return html\n",
    "  except urllib.HTTPError, e:\n",
    "    print e.code\n",
    "    print e.msg\n",
    "    return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_links(html, org_url):\n",
    "  '''\n",
    "  HTML 문서에 포함된 link 주소(URL)들의 목록을 리턴함\n",
    "  :param html: HTML 문서 내용\n",
    "  :param org_url: HTML 문서의 원래 웹주소. 문서내의 링크가 상대경로인 경우에 원래 웹주소와의 결합이 필요함(urljoin)\n",
    "  :return: the list of links\n",
    "  '''\n",
    "  soup = bs(html, 'lxml')\n",
    "  links = [urlparse.urljoin(org_url, link.get('href')) for link in soup.find_all('a')]\n",
    "  links = [link for link in links if link.startswith('http')]\n",
    "  return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text(html):\n",
    "  '''\n",
    "  HTML 문서에서 script, style 부분을 제외한 텍스트 내용을 리턴함.\n",
    "  :param html:\n",
    "  :return:\n",
    "  '''\n",
    "  soup = bs(html, 'lxml')\n",
    "  for s in soup('script'): s.extract()  # script, 자바스크립트 등 제거\n",
    "  for s in soup('style'): s.extract()  # CSS 등 style 코드 제거\n",
    "  for br in soup.find_all(\"br\"):\n",
    "    br.replace_with(\"\\n\")\n",
    "\n",
    "  cleanedlinelist = [' '.join(s.split()) for s in soup.strings]\n",
    "  filtered =  filter(len, cleanedlinelist)\n",
    "  return '\\n'.join(filtered)  # 화면에 출력해보려면  join()을 사용하여 리스트를  문자열로 변환.\n",
    "  #return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_clean_lines(text):\n",
    "  '''\n",
    "  글 내용 중에 불필요한 공백을 제거함.\n",
    "  1) 연속된 여러 공백(space,tab)은 하나의 공백(space)으로 변환,\n",
    "  2) 빈 라인(공백만 있는 라인)은 제거\n",
    "  :param text: 텍스트\n",
    "  :return: 깔끔한 텍스트\n",
    "  '''\n",
    "\n",
    "\n",
    "  res = [' '.join(line.split()) for line in text.splitlines()] # 각 라인의 앞뒤 공백 제거, 중간 공백둘은 한개의 space 로 변환. 리스트\n",
    "  res = filter(len, res) # 빈 라인(길이가 0인 것) 제거\n",
    "  return '\\n'.join(res)  # 뉴라인 포함된 문자열로 다신 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(weburl):\n",
    "  html = fetch(weburl)\n",
    "  text = get_text(html)\n",
    "  print text\n",
    "  #cleaned =  convert_to_clean_lines(text)\n",
    "  #print cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_daum_search_url(searchword):\n",
    "  '''\n",
    "  http://search.daum.net/search?\n",
    "  w=tot&\n",
    "  DA=YZR&\n",
    "  t__nil_searchbox=btn&\n",
    "  sug=&\n",
    "  sugo=&\n",
    "  q=%EC%8A%AC%EB%A1%9C%EC%9A%B0%EC%BA%A0%ED%8D%BC%EC%8A%A4&\n",
    "  tltm=kk43\n",
    "\n",
    "  :param searchword:\n",
    "  :return:\n",
    "  '''\n",
    "  kv = {\n",
    "    'w': 'tot',\n",
    "    'q': searchword\n",
    "  }\n",
    "  param = urllib.urlencode(kv)\n",
    "  return 'http://search.daum.net/search?' + param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_daum(word):\n",
    "  searchurl = build_daum_search_url(word)\n",
    "  html = fetch(searchurl)\n",
    "  links = get_links(html, searchurl)\n",
    "  print '\\n'.join(links)\n",
    "  text = get_text(html)\n",
    "  print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'HTTPError'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3de8a3c5caee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#if __name__ == '__main__':\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#  #main(myaddr)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msearch_daum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'슬로우캠퍼스'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-8b46340c566b>\u001b[0m in \u001b[0;36msearch_daum\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msearch_daum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m   \u001b[0msearchurl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_daum_search_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearchurl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m   \u001b[0mlinks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchurl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;32mprint\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-0e72e9b9719c>\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(web_url)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m#print len(html)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m   \u001b[1;32mexcept\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'HTTPError'"
     ]
    }
   ],
   "source": [
    "#if __name__ == '__main__':\n",
    "#  #main(myaddr)\n",
    "search_daum('슬로우캠퍼스')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
