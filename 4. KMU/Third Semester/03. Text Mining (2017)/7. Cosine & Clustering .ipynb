{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 코사인 유사도 & Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import quote_plus\n",
    "\n",
    "import requests\n",
    "import lxml.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_url = ('http://section.blog.naver.com/sub/SearchBlog.nhn?type=post&option.keyword={query}'\n",
    "              '&option.page.currentPage={page}&option.orderBy=sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%EB%94%A5%EB%9F%AC%EB%8B%9D'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = quote_plus('딥러닝')\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://section.blog.naver.com/sub/SearchBlog.nhn?type=post&option.keyword=%EB%94%A5%EB%9F%AC%EB%8B%9D&option.page.currentPage=2&option.orderBy=sim'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = search_url.format(query=query, page=2)\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = lxml.html.fromstring(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element a at 0x1bdc61a0c28>,\n",
       " <Element a at 0x1bdc61a0c78>,\n",
       " <Element a at 0x1bdc61a0cc8>,\n",
       " <Element a at 0x1bdc61a0d18>,\n",
       " <Element a at 0x1bdc61a0d68>,\n",
       " <Element a at 0x1bdc61a0db8>,\n",
       " <Element a at 0x1bdc61a0e08>,\n",
       " <Element a at 0x1bdc61a0e58>,\n",
       " <Element a at 0x1bdc61a0ea8>,\n",
       " <Element a at 0x1bdc61a0ef8>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.cssselect('h5 a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - a Class가 존재하지 않는다. 그럴때는 위에 태그를 본다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능, 머신러닝, 딥러닝 입문 http://patent21.blog.me/220976563267\n",
      "딥러닝을 위한 라이브러리 TensorFlow http://blog.naver.com/rkdwnsdud555?Redirect=Log&logNo=220988902977&from=section\n",
      "[도서]밑바닥부터 시작하는 딥러닝 http://blog.naver.com/civilize?Redirect=Log&logNo=220976431562&from=section\n",
      "[딥러닝] 네이버의 쿼리 어시스턴트 http://blog.naver.com/chatton03?Redirect=Log&logNo=220969790092&from=section\n",
      "머신러닝 딥러닝 학습을 위한 GPGPU 워크스테이션 http://blog.naver.com/wellmpc?Redirect=Log&logNo=220971162452&from=section\n",
      "밑바닥부터 시작하는 딥러닝 http://blog.naver.com/hyongsu44?Redirect=Log&logNo=220988314262&from=section\n",
      "딥러닝(Deep Learning) 실습과정 강좌 오픈! http://blog.officen.kr/220953268153\n",
      "우분투 딥러닝 학습을 위한 4Way GPU 워크스테이션 http://blog.naver.com/wellmpc?Redirect=Log&logNo=220962680123&from=section\n",
      "74.인공지능과 딥러닝 [2017.01.**~****.02.03] http://blog.naver.com/dudwo7970?Redirect=Log&logNo=220983998483&from=section\n",
      "[ML] 테리의 딥러닝 토크 (동영상) http://horajjan.blog.me/220961185493\n"
     ]
    }
   ],
   "source": [
    "for link in root.cssselect('h5 a'):\n",
    "    print(link.text_content(), link.attrib['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL 분해\n",
    " - 크롤링에서 필요했던 내용들을 분해해서 데이터를 format으로 하면 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import parse_qs, urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = urlparse('http://blog.naver.com/civilize?Redirect=Log&logNo=220976431562&from=section')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParseResult(scheme='http', netloc='blog.naver.com', path='/civilize', params='', query='Redirect=Log&logNo=220976431562&from=section', fragment='')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blog.naver.com'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.netloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/civilize'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Redirect=Log&logNo=220976431562&from=section'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Redirect': ['Log'], 'from': ['section'], 'logNo': ['220976431562']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs = parse_qs(result.query)\n",
    "qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'220976431562'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs['logNo'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post_url = 'http://blog.naver.com/PostView.nhn?blogId={}&logNo={}'.format(result.path[1:], qs['logNo'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://blog.naver.com/PostView.nhn?blogId=civilize&logNo=220976431562'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 게시물 내용 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post_res = requests.get(post_url)\n",
    "post_root = lxml.html.fromstring(post_res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\r\\n \\r\\n \\r\\n  밑바닥부터 시작하는 딥러닝 \\r\\n  \\r\\n  작가\\r\\n  사이토 고키\\r\\n  출판\\r\\n  한빛미디어\\r\\n  발매\\r\\n  2017.01.03.\\r\\n  평점\\r\\n  \\r\\n   \\r\\n    \\r\\n   \\r\\n  \\r\\n  \\r\\n  리뷰보기\\r\\n \\r\\n 이런 책이 진작 나왔다면 내가 머신러닝 공부하는데 장벽을 좀 덜 느꼈을텐데 말이다.왜 2017년에서야 나온거냐.일단 책 내용은 요즘 굉장히 핫한 뉴럴넷, 딥러닝을 파이썬으로 구현해 보면서 익히는 것이고,책 제목대로 초짜들도 따라하면서 뉴럴넷의 원리와 필요한 지식들을 배울 수 있게 구성되어 있다.뭐 그래도 calculus, linear algebra 정도는 머릿속에 배경지식을 깔고 들어가야 하고,애초에 머신러닝 분야가 통계나 수학적인 백그라운드 없으면 쉽게 할 수 없으니까그정도는 공부하고 읽는것을 추천한다.근데 난 이 내용 석사때 분명 배우긴 배웠고, matlab으로 구현까지 했던 기억이 있는데하도 손을 놔서 그런지 다 까묵어서 리마인드하는 용도로 보고 있기는 하다.그런 용도로도 괜찮은 책인것도 같고.\\r\\n\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_root.cssselect('div#postViewArea')[0].text_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 블로그 스크래핑 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keyword = '딥러닝'\n",
    "query = quote_plus(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_url = ('http://section.blog.naver.com/sub/SearchBlog.nhn?type=post&option.keyword={query}'\n",
    "              '&option.page.currentPage={page}&option.orderBy=sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "posts = []\n",
    "for page in tqdm.tqdm_notebook(range(1, 20)):\n",
    "    url = search_url.format(query=query, page=page)\n",
    "    res = requests.get(url)\n",
    "    root = lxml.html.fromstring(res.text)\n",
    "    \n",
    "    for link in root.cssselect('h5 a'):\n",
    "        link_url = link.attrib['href']\n",
    "\n",
    "        # 다른 형식의 주소는 무시\n",
    "        if not link_url.startswith('http://blog.naver.com'):\n",
    "            continue\n",
    "\n",
    "        # 진짜 주소\n",
    "        result = urlparse(link_url)\n",
    "        blog_id = result.path[1:]\n",
    "        qs = parse_qs(result.query)\n",
    "        post_id = qs['logNo'][0]\n",
    "        post_url = 'http://blog.naver.com/PostView.nhn?blogId={}&logNo={}'.format(blog_id, post_id)\n",
    "        \n",
    "        # 본문 가져오기\n",
    "        post_res = requests.get(post_url)\n",
    "        post_root = lxml.html.fromstring(post_res.text)\n",
    "        \n",
    "        try:\n",
    "            body = post_root.cssselect('div#postViewArea')[0]\n",
    "            posts.append(body.text_content())\n",
    "        except IndexError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV로 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('posts.csv', 'w', encoding='utf8', newline='') as f:\n",
    "    w = csv.writer(f)\n",
    "    for post in posts:\n",
    "        post_short = re.sub(r'\\s+', ' ', post)  # 모든 종류의 공백을 빈 칸 하나로 바꿈 (엑셀에서 보기 좋게)\n",
    "        w.writerow([post_short])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV에서 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts = []\n",
    "with open(\"posts.csv\", encoding=\"cp949\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        post = row[0]\n",
    "        if len(post) > 100:\n",
    "            posts.append(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 머신러닝의 딥러닝, 미래사회를 이끌다 산업 현장의 지각 변동을 이룰 미래 기술 미국의 IT 리서치 기업 가트너 그룹은 지난 2016년, ITxpo 2016에서 향후 5년간 혁신 잠재력을 지닌 10대 전략기술 트렌드를 발표했다. 가트너가 선정한 2017년 10대 전략 기술은 인텔리전트(Intellegent)와 디지털(Digital), 그리고 메시(Mesh)라는 3가지 영역으로 구분된다. 그중 인공지능과 고급 기계 학습, 즉 머신러닝은 10대 기술의 서두로서 로봇, 자율주행자동차, 가전 기기 등 물리적 디바이스를 비롯해 미래 IT 산업을 선도할 핵심기술로 평가 받고 있다.세상을 바꿀 최고의 전략 기술지난 2016년 ‘ITxpo 2016’에서 미국의 가트너 그룹이 발표한 ‘2017년 10대 전략기술 트렌드’는 향후 5년간 IT 산업 발전을 이끌 10가지 핵심 기술에 대해 이야기한다. 그중 인텔리전트(Intellegent), 즉 지능의 영역으로 분류되는 ‘인공지능과 고급 기계 학습(AI & Advanced Machine Learning)’은 기존의 알고리즘을 넘어 학습 예측, 적응을 비롯해 잠재적으로 스스로 가동하는 자율시스템을 만들어 지능형 스마트 기기를 만든다.인공지능의 한 분야인 머신러닝(Machine Learning)은 데이터의 생성량과 주기, 형식 등 방대한 빅데이터들을 분석해 미래를 예측하는 기술을 말한다. 이 기술은 기존의 빅데이터 분석과 유사점을 지녔지만, 데이터를 수집·분석해 미래를 예측한다는 점에서 차이점을 지녔다. 딥러닝, 신경망, 자연어처리, 첨단기법 등 기술의 결합으로 완성된 머신러닝 기술은 최근 각광받고 있는 물리적 디바이스(로봇, 자율주행자동차 등)을 비롯해 지능형 앱, 메시 디바이스 등 다양한 분야에 적용·융합될 혁신 기술로 평가받는다. 대표적 사례로 글로벌 소셜 네트워크 서비스를 제공하는 페이스북은 ‘Project PANDA’를 통한 딥페이스 기술을 연구한다. 또한, 세계 최대 비디오 스트리밍 기업 넷플릭스는 사용자의 구매 이력을 토대로 영화를 추천해 주는 서비스에 머신러닝을 적용했다. 이러한 기업들의 머신러닝 적용 배경에선 AI 기술의 정점으로 평가받는 딥러닝(Deep Learning)이 있었다. 머신러닝의 한 분야인 딥 러닝은 인공지능의 핵심 기술로서 인간의 두뇌 신경망 구조를 모방한 인공 신경망을 바탕으로 두각을 나타내고 있다. 그중 이미지 인식 분야에서 두드러지는 딥러닝 기술은 2014년부터 페이스북의 딥페이스 서비스에 적용되며 그 가능성을 인정받은 바 있다. 인공 신경망 기반 기계 학습 기술인 딥러닝은 컴퓨터가 스스로 학습한다는 점에서 머신러닝과 동일성을 지녔다. 하지만 데이터의 특징에 대한 정보를 사람이 직접 제공해야 하는 머신러닝과 달리 딥러닝은 특징을 스스로 파악해 분류한다는 점에서 차이점을 지녔다. 현대 지능 기술의 정점이 될 것으로 예견되는 두 기술은 다양한 산업현장에 적용 및 활용된다. 실현화 단계로 돌입한 딥러닝, 그리고 머신러닝의 미래딥러닝을 이용한 이미지 인식 기술의 인식률(97.35%)은 사람의 평균 인식률(97.5%)과 동일한 수준으로 발전했다. 또한, 다양한 서비스 추천 기능에도 적용되고 있는 딥러닝 기술은 인터넷 쇼핑몰 사이트의 상품 추천 서비스는 물론, 개인화된 음악 추천 서비스, 라디오 서비스 등에서도 엿볼 수 있다. 또한, 금융 분야에서 주목받는 딥러닝 기술은 미국의 온라인 결제 서비스 페이팔의 이상 금융거래 탐지시스템(FDS)에 적용되며 온라인 결제 패턴을 지능적으로 수집·분석해 기존 형태와 다른 패턴에서 범죄의 여부를 분류해 낸다. 금융 기업들에게 필요한 데이터 분석으로 주가와 기업 부도 예측이 가능하다는 점은 또 하나의 시사점이다. 머신러닝의 역사는 1950년부터 이어진다. 70년에 가까운 역사 속에서 정체되어 있던 기술은 2000년대 중반에 이르러서야 발전이 이루어지고 있다. 실제 머신러닝의 발전을 이끈 딥러닝의 인공신경망 기술은 IT 분야에서 새로운 것으로 보기는 힘들다. 전문가들은 ‘지난 20년간 인터넷 기술의 발전으로 축적된 방대한 정보와 이를 처리하기 위한 컴퓨터의 연산 능력 향상이라는 두 가지 요소가 만들어 낸 성과’라고 입을 모았다. 최근 딥러닝을 통한 머신러닝의 성공이 만들어낸 변화는 인류의 편의성을 혁신적으로 증대시킬 수 있다는 점에서 각광받고 있다. 또한, 구글, 페이스북 등 글로벌 IT 기업들은 딥러닝 기술 개발에 주력하며 끝없는 경쟁을 이어가고 있다. 구글은 학계에서 딥러닝 분야의 최고 석학으로 평가 받는 토론토 대학의 제프리 힌튼 교수를 영입해 자율주행차와 번역 서비스에 기술을 적용하고 있다. 검색 엔진 분야에서 세계 최고의 점유율을 지닌 만큼 구글이 지닌 방대한 데이터와 이미지들은 딥러닝 기술의 연구·개발을 위한 최고의 경쟁력으로 평가받고 있다. 한편, 페이스북은 힌튼 교수의 제자인 뉴욕주립대학교의 얀 레쿤 교수를 영입해 딥러닝을 광고에 반영하는 기술을 개발하고 있다. 이처럼 기업들의 경쟁을 바탕으로 성장 중인 딥러닝을 통해 추후 이미지 인식 분야를 토대로 커뮤니케이션이 가능한 로봇의 출현이 예고되고 있으며, 각종 언어에 대한 상황판단과 문화적 맥락의 수집을 통해 언어장벽을 무너뜨릴 통번역 서비스를 만들어 낼 것으로 예상되고 있다. 하지만 IT 강국으로 평가 받는 국내 사회에서는 머신러닝 기술과 딥러닝에 대한 관심이 부족한 점이 사실이다. 미래의 전략 트렌드로서 주목받는 머신 러닝과 이를 보조하는 딥러닝 기술이 만들어낼 새로운 사회에 국내 기업들과 관련 정부 부처의 관심이 필요한 시점이다.? 이슈메이커 이민성 기자 jadelee@issuemaker.kr ',\n",
       " ' 인공지능 레벨1. 가전제품을 이용한 단순 제어 프로그램2. 장기나 청소로봇 같은 다채로운 프로그램3. 빅데이터를 이용한 프로그램4. 기계학습, 딥러닝을 이용한 프로그램제1차 AI 붐 추론, 탐색의 시대제2차 AI 붐 지식의 시대제3차 AI 붐 기계학습, 특징 표현 학습미로 풀기 -> 막다른 길에 다다르면 전의 코너에서 이동히노이의 탑 -> 작은 원반 위 큰 원반을 올릴 수 없다.(원반마다 가중치나 순위를 매겨 정하면 될듯)체스 -> 최소 최대 법 (2수 앞 예측)1. 경우의 수를 수치로 표현2. 경우의 수의 수치가 최소인 것 선택3. 최소중 가장 나은 것 선택몬테 카를로법 -> 수의 계산을 포기하고 랜덤으로 둔다사야 프로젝트 -> 노드를 이용해 관계 표시추이율 성립 -> 1<3, 3<7, 1<7 (O)(is-a) -> 가위<바위, 바위<보, 보<가위 (X)헤비 웨이트 온톨로지 -> 지식을 기술 방법, 인간이 직접 개입라이트 웨이트 온톨로지 -> 지식을 기술 방법, 컴퓨터가 자동으로심볼 그라운딩 문제 -> 볼과 그것을 의미하는 것이 결부프레임 문제로봇, 동굴, 배터리, 시한폭탄로봇 1: 동굴에서 배터리를 가져옴 (시한폭탄도 같이 가져옴)로봇 2: 부차적으로 일어나는 것도 고려 (동굴에서 생각하다 터짐)로봇 3: 관계없는 사항은 고려 않도록 (동굴에도 못 들어감)기계학습(학습하다->분류하다)1. 지도학습-> 입력화 출력2. 비지도학습 -> 데이터만 입력 (패턴을 추출)1. 최근접 이웃 방법->가장 가까운 이웃의 분류에 따른다.단점: 정치 카테고리에 문화가 섞이면 오버 피팅 결과2. 나이브 베이즈 법->베이즈의 정리 사용(단어가 포함되는 확률로 카테고리별 점수를 매겨 분류)3. 결정 트리->판단 기준으로 O, X를 이용해 판별단점: 복잡한 문제에서 정밀도가 높지 않다.4. 서포트 벡터 머신->데이터를 구분 짓는 구분선과 그룹 간의 간격을 최대로 나눈다.장/단점: 정밀도가 높다. 큰 데이터는 오래 걸린다.5. 뉴럴 네트워크->인간의 뇌신경을 흉내 임계값 기준으로 신호 발화 1, 아니면 0오차역 전파 (가중치의 조정을 되풀이하고 인식의 정밀도 상승)->전체의 오차가 작게 기울기 값을 잡는다. 가중치를 이용해 오차를 잡는다.기계학습의 문제->무엇을 특징으로 할지 인간이 정하지 않으면 안 된다.인공지능의 문제(특징을 꺼내 개념[시니피어]와 이름[시니피앙]을 주면 된다)->개념을 스스로 획득할 수 없었다.딥러닝 <-심층학습->데이터를 바탕으로 스스로 특징을 만들어 낸다.계층을 포개어 탐구하다->상관이 있는 것을 한 묶음으로 해서 특징을 꺼내고, 그것을 이용해서 더 높은 차원의 특징을 꺼낸다.사용 개념1. 오토인코더 입력층 ↔ 은닉층 ↔ 출력층2. 은닉층을 다음 레이어로 은닉층 ↔ 입력층 은닉층 ↔ 출력층3.입력층 = 출력층 은닉층 ↔ 입력층 (같으므로 출력층 제거)4. 임계층 생성 은닉층 3층 ↔ 은닉층 2층 ↔ 은닉층 1층 ↔ 입력층(※은닉층은 다음 측의 입력층이 됨) (은닉층 3층의 입력층) (은닉층 2층의 입력층)?손 글씨, 지문 인식 등 (대표적으로 구글의 고양이 인식) 딥러닝에서의 기술 진전① 이미지 특징의 추상화② 멀티 모달한 추상화 (사진과 같은 이미지뿐만 아니라 동영상, 소리, 냄새까지도 인지 가능한)③ 행동 결과의 추상화 (행동에 대한 결과를 생각 가능, 행동의 계획이 세워짐)④ 일련의 행동을 통해 현실세계에서의 특징 추출⑤ 언어와 개념의 그라운딩⑥ 언어를 통해서 지식 획득싱귤래리터 ->인공지능의 자신의 능력을 넘는 인공지능을 스스로 만들어 낼 수 있는 시점 ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDM 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Komoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_noun(text):\n",
    "    nouns = tagger.nouns(text)\n",
    "    return [n for n in nouns if len(n) > 1]  # 2글자 이상인 명사만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=get_noun, max_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdm = cv.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDM 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('blog_tdm.npz',tdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 목록 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('blog_tdm.json', \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(cv.get_feature_names(),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDM 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load('blog_tdm.npz', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arr_0']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdm = data['arr_0'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<534x500 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 26169 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 목록 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('blog_tdm.json', encoding='utf8') as f:\n",
    "    words = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['가격', '가중치', '가지', '가치', '감사', '감소', '감정', '강의', '강좌', '강화']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 차원 축소 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = svd.fit_transform(tdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(534, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<534x500 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 26169 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm # 534파일을 500개 단어로 표현 했었다. 그걸 위에서 10개 차원으로 변경 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 34.0023001 , -10.80033207,   3.64908666, -23.39763816,\n",
       "        -6.00303814,  14.37208342,   1.65445348, -13.37121719,\n",
       "         3.9279552 ,   1.18865712])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규화 \n",
    " - 길이도 정보를 가지고 있는데 왜 줄이느냐 라는 비판도 있다.\n",
    "  - 보통 많이 사용한다. \n",
    "  - 다른걸 한다고 해서 크게 좋아지는건 없다..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer) \n",
    "# 연속으로 적용할 수 있도록 svd -> normalizer가 될 수 있도록 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = lsa.fit_transform(tdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(534, 10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7127271 , -0.22638801,  0.07648591, -0.49043818, -0.12589737,\n",
       "        0.30123156,  0.03486466, -0.28154767,  0.0818862 ,  0.0246629 ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pos[0, :] ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 클러스터링 \n",
    " - 1차로 임의의 점으로 cluster 갯수로 해서 가까운 놈들로 나눈다.\n",
    " - cluster 에 나누어진 놈들끼리 평균을 구하면 다시 중심을 구할 수 있다.\n",
    " - 또 다시 평균을 내서 중심을 잡는다. \n",
    " - 그리고 점들을 중심에서의 거리를 구하고 가까운 놈들에게 포함시키게 한다.\n",
    " - 변화가 없을때 까지 돌린다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=5, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 4, 1, 3, 3, 2, 4, 1, 1, 1, 3, 0, 0, 4, 4, 4, 1, 4, 3, 3, 4, 2,\n",
       "       2, 2, 3, 2, 4, 4, 1, 3, 2, 0, 3, 3, 0, 2, 3, 0, 0, 1, 0, 1, 2, 3, 1,\n",
       "       3, 4, 1, 1, 1, 4, 3, 3, 3, 1, 3, 0, 2, 4, 2, 2, 3, 3, 2, 0, 2, 3, 2,\n",
       "       2, 0, 1, 1, 1, 0, 1, 2, 1, 3, 3, 3, 1, 2, 3, 3, 3, 0, 3, 0, 4, 2, 3,\n",
       "       0, 3, 0, 2, 3, 2, 3, 0, 3, 2, 1, 0, 4, 2, 0, 1, 3, 0, 2, 1, 3, 3, 4,\n",
       "       3, 3, 1, 3, 4, 0, 2, 1, 0, 0, 0, 1, 2, 3, 3, 0, 3, 1, 4, 1, 0, 0, 3,\n",
       "       1, 0, 4, 2, 2, 2, 1, 2, 0, 3, 3, 1, 1, 2, 1, 1, 4, 3, 0, 1, 1, 3, 1,\n",
       "       1, 0, 0, 4, 0, 0, 1, 1, 1, 0, 2, 1, 3, 1, 1, 3, 0, 3, 3, 3, 3, 3, 2,\n",
       "       4, 4, 2, 2, 3, 1, 0, 0, 3, 1, 0, 3, 0, 3, 4, 1, 2, 1, 3, 4, 4, 4, 3,\n",
       "       1, 0, 0, 3, 3, 0, 2, 3, 3, 0, 1, 1, 0, 4, 0, 3, 4, 0, 2, 0, 2, 1, 0,\n",
       "       2, 3, 0, 3, 3, 0, 2, 2, 3, 2, 3, 3, 2, 1, 1, 1, 2, 1, 1, 3, 0, 2, 0,\n",
       "       4, 0, 0, 3, 4, 3, 1, 0, 2, 0, 1, 3, 0, 2, 1, 2, 2, 1, 4, 3, 2, 2, 3,\n",
       "       4, 3, 0, 4, 0, 0, 1, 3, 3, 1, 2, 3, 2, 1, 3, 2, 0, 3, 2, 3, 3, 4, 2,\n",
       "       1, 3, 0, 0, 4, 1, 3, 0, 2, 0, 2, 0, 0, 0, 1, 3, 0, 2, 2, 0, 0, 3, 1,\n",
       "       3, 3, 0, 3, 3, 2, 2, 2, 4, 1, 3, 0, 3, 1, 2, 4, 3, 2, 0, 3, 3, 3, 0,\n",
       "       1, 2, 2, 4, 3, 3, 4, 3, 1, 4, 4, 2, 1, 0, 1, 4, 1, 4, 0, 2, 3, 2, 1,\n",
       "       4, 1, 3, 1, 4, 4, 0, 2, 2, 1, 3, 1, 3, 1, 0, 0, 3, 3, 3, 3, 3, 0, 3,\n",
       "       4, 3, 0, 0, 1, 1, 3, 0, 0, 1, 3, 3, 3, 4, 2, 2, 1, 0, 0, 4, 3, 0, 4,\n",
       "       3, 2, 0, 3, 4, 1, 4, 2, 1, 0, 0, 0, 2, 0, 3, 3, 2, 0, 1, 0, 1, 2, 3,\n",
       "       4, 3, 3, 3, 0, 0, 2, 4, 2, 3, 3, 3, 3, 2, 3, 0, 4, 4, 3, 2, 4, 3, 0,\n",
       "       3, 1, 1, 1, 1, 3, 0, 3, 2, 2, 2, 3, 1, 3, 3, 3, 1, 0, 3, 0, 3, 4, 3,\n",
       "       3, 0, 2, 0, 3, 1, 3, 4, 2, 3, 1, 3, 1, 3, 1, 0, 4, 1, 2, 2, 3, 3, 0,\n",
       "       4, 0, 0, 0, 1, 2, 1, 0, 1, 1, 2, 1, 2, 3, 1, 2, 0, 3, 1, 4, 4, 1, 1,\n",
       "       1, 0, 1, 1, 4])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 인공지능 레벨1. 가전제품을 이용한 단순 제어 프로그램2. 장기나 청소로봇 같은 다채로운 프로그램3. 빅데이터를 이용한 프로그램4. 기계학습, 딥러닝을 이용한 프로그램제1차 AI 붐 추론, 탐색의 시대제2차 AI 붐 지식의 시대제3차 AI 붐 기계학습, 특징 표현 학습미로 풀기 -> 막다른 길에 다다르면 전의 코너에서 이동히노이의 탑 -> 작은 원반 위 큰 원반을 올릴 수 없다.(원반마다 가중치나 순위를 매겨 정하면 될듯)체스 -> 최소 최대 법 (2수 앞 예측)1. 경우의 수를 수치로 표현2. 경우의 수의 수치가 최소인 것 선택3. 최소중 가장 나은 것 선택몬테 카를로법 -> 수의 계산을 포기하고 랜덤으로 둔다사야 프로젝트 -> 노드를 이용해 관계 표시추이율 성립 -> 1<3, 3<7, 1<7 (O)(is-a) -> 가위<바위, 바위<보, 보<가위 (X)헤비 웨이트 온톨로지 -> 지식을 기술 방법, 인간이 직접 개입라이트 웨이트 온톨로지 -> 지식을 기술 방법, 컴퓨터가 자동으로심볼 그라운딩 문제 -> 볼과 그것을 의미하는 것이 결부프레임 문제로봇, 동굴, 배터리, 시한폭탄로봇 1: 동굴에서 배터리를 가져옴 (시한폭탄도 같이 가져옴)로봇 2: 부차적으로 일어나는 것도 고려 (동굴에서 생각하다 터짐)로봇 3: 관계없는 사항은 고려 않도록 (동굴에도 못 들어감)기계학습(학습하다->분류하다)1. 지도학습-> 입력화 출력2. 비지도학습 -> 데이터만 입력 (패턴을 추출)1. 최근접 이웃 방법->가장 가까운 이웃의 분류에 따른다.단점: 정치 카테고리에 문화가 섞이면 오버 피팅 결과2. 나이브 베이즈 법->베이즈의 정리 사용(단어가 포함되는 확률로 카테고리별 점수를 매겨 분류)3. 결정 트리->판단 기준으로 O, X를 이용해 판별단점: 복잡한 문제에서 정밀도가 높지 않다.4. 서포트 벡터 머신->데이터를 구분 짓는 구분선과 그룹 간의 간격을 최대로 나눈다.장/단점: 정밀도가 높다. 큰 데이터는 오래 걸린다.5. 뉴럴 네트워크->인간의 뇌신경을 흉내 임계값 기준으로 신호 발화 1, 아니면 0오차역 전파 (가중치의 조정을 되풀이하고 인식의 정밀도 상승)->전체의 오차가 작게 기울기 값을 잡는다. 가중치를 이용해 오차를 잡는다.기계학습의 문제->무엇을 특징으로 할지 인간이 정하지 않으면 안 된다.인공지능의 문제(특징을 꺼내 개념[시니피어]와 이름[시니피앙]을 주면 된다)->개념을 스스로 획득할 수 없었다.딥러닝 <-심층학습->데이터를 바탕으로 스스로 특징을 만들어 낸다.계층을 포개어 탐구하다->상관이 있는 것을 한 묶음으로 해서 특징을 꺼내고, 그것을 이용해서 더 높은 차원의 특징을 꺼낸다.사용 개념1. 오토인코더 입력층 ↔ 은닉층 ↔ 출력층2. 은닉층을 다음 레이어로 은닉층 ↔ 입력층 은닉층 ↔ 출력층3.입력층 = 출력층 은닉층 ↔ 입력층 (같으므로 출력층 제거)4. 임계층 생성 은닉층 3층 ↔ 은닉층 2층 ↔ 은닉층 1층 ↔ 입력층(※은닉층은 다음 측의 입력층이 됨) (은닉층 3층의 입력층) (은닉층 2층의 입력층)?손 글씨, 지문 인식 등 (대표적으로 구글의 고양이 인식) 딥러닝에서의 기술 진전① 이미지 특징의 추상화② 멀티 모달한 추상화 (사진과 같은 이미지뿐만 아니라 동영상, 소리, 냄새까지도 인지 가능한)③ 행동 결과의 추상화 (행동에 대한 결과를 생각 가능, 행동의 계획이 세워짐)④ 일련의 행동을 통해 현실세계에서의 특징 추출⑤ 언어와 개념의 그라운딩⑥ 언어를 통해서 지식 획득싱귤래리터 ->인공지능의 자신의 능력을 넘는 인공지능을 스스로 만들어 낼 수 있는 시점 '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Deep learning 을 위한 TensorFlow 텐서플로우는 원래 구글의 인공지능 관련 연구팀인 Google brain팀에서 소속 연구원 및 엔지니어들이 머신러닝, 딥러닝 연구를 목적으로 만들어진 라이브러리이다. 텐서플로우는 노드(Node)와 엣지(Edge)로 구성된 Data flow graph 형태의 구조로 수치 계산을 위해 사용된다. 그래프의 노드는 수학적 연산 역할을 하고, 노드 사이에 연결된 엣지는 연결된 노드들 사이에서 전달되는 텐서(다차원 데이터 배열)의 역할을 한다. 텐서플로우는 별도의 라이브러리 없이 하나 이상의 CPU와 GPU를 적절히 분산시켜 사용할 수 있다는 것이 큰 장점이다.(https://www.tensorflow.org) 텐서플로우를 이용하는 세계적인 기업들 텐서플로우의 아키텍쳐를 살펴보자 (출처:https://www.tensorflow.org/extend/architecture)텐서플로우 런타임은 cross-platform 라이브러리로 C++, Python, Java, GO 등 여러 언어에서 사용할 수 있다.구조는 역할에 따라 크게 Client, Distributed master, Worker Service, Kernel implementation 으로 나눠진다.Client는 수행해야할 계산을 그래프의 흐름으로써 정의한다. 그리고 세션을 이용해 그래프 계산을 시작한다. Distributed master는 부분 그래프 계산 작업을 서로 다른 장치 및 프로세서에서 실행될 수 있도록 분할 해준다.즉, 그래프의 부분을 다시 여러 태스크로 나눠 일을 분배해주는 것이다. 일을 쪼개고 나면 각 Worker service에게 일을 나눠서 배포한다.그리고 Worker service의 일을 시작하게 명령하는 역할도 담당한다.Worker service는 일이 할당되면 사용가능한 하드웨어(CPU,GPU 등)에 적절한 Kernel implementation을 사용하여 할당 받은 그래프 작업을 실행할 수 있도록 스케줄링한다.Kernel implementation이 실질적으로 각 그래프 작업에 대한 계산을 수행하게 된다. 그리고 결과는 상위단계로 전달된다.각 컴포넌트별 자세한 동작 방식은 텐서플로우 공식 홈페이지에서 확인할 수 있다.https://www.tensorflow.org/extend/architecture현재 텐서플로우 외에도 딥러닝을 위한 다양한 라이브러리가 존재한다. caffe, keras, Java용 라이브러리 ND4J 등 많은 라이브러리가 제공되고 있다. 이 중 무엇을 사용해야할 지 고민이 될 수도 있다. (출처:https://twitter.com/fchollet/status/830499993450450944)고맙게도 누군가 github에 등록된 프로젝트들의 정보를 이용해 각 딥러닝 라이브러리의 사용정도를 비교하기 좋게 분석해주었다.딥러닝을 시작하는 입장에서 라이브러리 선택은 자유지만 이왕이면 앞으로도 많은 사람들에 의해 사용되고, 발전 가능성이 있는 라이브러리를 선택하는게 좋을 것이라고 판단된다. '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
