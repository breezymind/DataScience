{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word Embedding \n",
    "- Embed : 어디에 부여하다 라는 의미, \n",
    "- 단어에 좌표를 부여하는 것\n",
    "- 단어의 의미를 수치화하는 것으로 생각\n",
    "- 텍스트 처리가 더 빠르고 정확해짐\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 . Latent Semantic Analysis (LSA)\n",
    " - Term-Document Matrix를 PCA\n",
    " - 빈도가 함께 증가/감소하는 관계의 단어들을 축으로 삼음\n",
    " - **적은 데이터**에서도 잘 작동\n",
    " - 어순 고려 X (TDM의 경우와 같이 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 어순 \n",
    " - 나, 는, 오늘, 밥, 을, ?\n",
    " - ?자리에 들어갈 수 있는 말은 한정: 먹었다, 굶었다 등등...\n",
    " - 어순이 중요 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NNLM\n",
    " - Feedforward Neural Net Language Model\n",
    " - Bengio et al. (2003)\n",
    "![img1](img/0527/1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 NNLM이 푸는 문제 \n",
    "![img2](img/0527/2.PNG)\n",
    "- 앞에 나온 단어들로 ?에 들어갈 단어를 예측\n",
    "- 앞에 나온 단어 / 뒤에 나온 단어 / 앞뒤로 나온 단어 등도 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 NNLM\n",
    "![img2](img/0527/3.PNG)\n",
    "![img2](img/0527/4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 one-hot encoding\n",
    " - 100 종류의 단어가 있다면\n",
    " - 각 단어에 1부터 100까지 번호를 붙임\n",
    " - 길이 100인 벡터를 모두 0으로 채움\n",
    " - 38번 단어 -> 38번째 값을 1로 표시\n",
    " - 모든 단어를 위와 같은 방법으로 벡터로 표현\n",
    " - 예) 나(16) 는(94) 밥(27) 을(36) -> 이걸 각각을 100개 열을 가진 One-hot \n",
    "![img5](img/0527/5.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 벡터에 행렬을 곱하면? \n",
    "- 뒤의 [0,1,0,0] -> one-hot으로 만든 것.\n",
    "- 메트릭스를 곱해서 One-Hot을 압축적으로 표현이 가능하도록 하는 것.\n",
    "- One-Hot 에서 두 단어의 거리를 모두 $\\sqrt2$\n",
    "![img6](img/0527/6.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 NNLM\n",
    "![img7](img/0527/7.PNG)\n",
    "![img8](img/0527/8.PNG)\n",
    " - 나는 ?을 먹었다.\n",
    "  - ?에 밥, 라면 의미가 연관성이 강하게 있으므로 비슷한 좌표에 있어야 출력이 비슷해 질 것이다. 결국 입력이 비슷해져야 출력도 비슷해 진다. \n",
    "  - 좌표가 비슷한 단어들끼리 배치가 되어 압축된 공간에 존재하게 될것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 학습 \n",
    " - 텍스트를 넣고 신경망을 학습시키면\n",
    " - 대상 단어를 잘 예측하도록 행렬 C가 구해짐\n",
    " - 단어에 행렬 C를 곱한 결과를 사용\n",
    " \n",
    "## 4.7 NNLM\n",
    "![img9](img/0527/9.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 NNLM의 문제점 \n",
    " - 학습하기가 너무 어렵다. Parameters가 너무 많다.\n",
    "![img10](img/0527/10.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    " - Mikolov et al. (2013)\n",
    " - NNLM의 학습 효율을 높임\n",
    "  - 기본 구조는 NNLM이다. \n",
    " - CBOW와 Skip-gram 제시\n",
    "![img11](img/0527/11.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CBOW와 Skip-gram\n",
    " - CBOW : 주변 단어로 그 단어를 예측\n",
    " - Skip-gram : 하나의 단어로 주변 단어 예측.\n",
    "  - 경험적으로 Skip-gram이 더 잘된다고 한다. \n",
    "![img12](img/0527/12.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 CBOW\n",
    " - Continuous Bag-Of-Words\n",
    " - 주변 단어로 대상 단어 예측\n",
    " - 은닉층을 단순합으로 대체"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 NNLM vs CBOW\n",
    " - NNLM\n",
    "![img13](img/0527/13.PNG)\n",
    " - CBOW\n",
    "![img14](img/0527/14.PNG)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Skip-gram\n",
    " - CBOW와 반대로 대상 단어로 주변 단어를 예측 \n",
    "\n",
    "## 5.4 결과\n",
    " - 더 적은 데이터로 더 큰 벡터를 만들 수 있음\n",
    " - 예측도 비슷하거나 더 정확\n",
    "\n",
    "![img15](img/0527/15.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    " - 목적에 상관없이 학습이 가능하므로, 다양한 곳에다가 계속적으로 사용가능하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Word2Vec의 특이한 성질\n",
    "![img16](img/0527/16.PNG)\n",
    "![img17](img/0527/17.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Word2Vec의 활용\n",
    "### 7.1 Word Embedding으로 활용\n",
    " - 감정 분석, 문서 분류 등을 수행할 때\n",
    " - 단어 -> Word Embedding으로 먼저 변환\n",
    " - 대부분의 경우 성능 향상\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 텍스트 이외 분야에서 활용\n",
    " - 텍스트: 단어가 순서대로 있는 것\n",
    " - (     )이 순서대로 있는 것이면 Word2Vec을 활용 가능\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 단백질 \n",
    "![img18](img/0527/18.PNG)\n",
    " - 인체는 단백질은 20종의 아미노산으로 구성\n",
    " - 아미노산의 순서에 따라 다른 단백질\n",
    " - 아미노산:단백질 = 단어:텍스트\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 ProtVec\n",
    " - Asgari & Mofad (2015)\n",
    " - 아미노산을 3개씩 묶어 '단어'로 만듦\n",
    " - Skip-gram을 이용해 3-아미노산 '단어'를 벡터로 만듦\n",
    " - 단백질은 ‘단어’의 합으로 표현\n",
    "![img19](img/0527/19.PNG)\n",
    " - SVM을 이용해 문제 단백질(FG-Nups)과 일반 단백질 구별\n",
    " - 정확도 99.81%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = requests.get('https://www.gutenberg.org/files/2591/2591-0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grimm = res.text[2801:530661]  # 앞과 뒤의 불필요한 부분 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grimm = re.sub(r'[^a-zA-Z\\. ]', ' ', grimm)  # 불필요한 문자 제거 알파벳, 빈칸, 마침표 빼고 다 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = grimm.split('. ')  # 문장 단위로 자름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE',\n",
       " 'GOLDEN',\n",
       " 'BIRD',\n",
       " 'A',\n",
       " 'certain',\n",
       " 'king',\n",
       " 'had',\n",
       " 'a',\n",
       " 'beautiful',\n",
       " 'garden',\n",
       " 'and',\n",
       " 'in',\n",
       " 'the',\n",
       " 'garden',\n",
       " 'stood',\n",
       " 'a',\n",
       " 'tree',\n",
       " 'which',\n",
       " 'bore',\n",
       " 'golden',\n",
       " 'apples']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [s.split() for s in sentences] # 여기에 한글 Konlp 형태소 분석 사용 하면 될듯. \n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\py35\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(data,         # 리스트 형태의 데이터\n",
    "                 sg=1,         # 0: CBOW, 1: Skip-gram\n",
    "                 size=100,     # 벡터 크기\n",
    "                 window=5,     # 고려할 앞뒤 폭(앞뒤 2단어)\n",
    "                 min_count=5,  # 사용할 단어의 최소 빈도(5회 이하 단어 무시)\n",
    "                 workers=4)    # 동시에 처리할 작업 수(코어 수와 비슷하게 설정)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 저장 & 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어를 벡터로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09505583,  0.04000263,  0.03499569, -0.06103792, -0.13397843,\n",
       "        0.095457  , -0.12989151,  0.161073  ,  0.17839022,  0.03679248,\n",
       "        0.13984223, -0.01845713,  0.14250068, -0.15325513,  0.02942706,\n",
       "        0.13004041,  0.17245889,  0.10079227,  0.0179996 ,  0.17999029,\n",
       "       -0.10285883,  0.1610191 ,  0.07076557,  0.10476649, -0.08188551,\n",
       "       -0.09458034, -0.14678054,  0.17034711, -0.06835218,  0.02412416,\n",
       "       -0.0337302 , -0.22222732, -0.14069764, -0.12943645,  0.11881431,\n",
       "        0.0621578 ,  0.17629775, -0.22417469,  0.12419313,  0.03614515,\n",
       "        0.07139631, -0.00614704, -0.01083947, -0.1516926 , -0.25641394,\n",
       "        0.0931441 , -0.02510201,  0.28055406,  0.17166962,  0.01700616,\n",
       "       -0.00955569,  0.06398132, -0.15578115, -0.08290041,  0.30363736,\n",
       "       -0.07593025, -0.0289566 ,  0.06906704, -0.0673705 , -0.12984982,\n",
       "       -0.09408101,  0.00775313,  0.09219365, -0.11952789, -0.12448721,\n",
       "        0.39417014,  0.01816131, -0.00401243,  0.17750902,  0.01709219,\n",
       "       -0.00816275,  0.11727729, -0.07650723, -0.13846618,  0.01689572,\n",
       "       -0.27829289, -0.11437648, -0.27834573,  0.12154798,  0.26533926,\n",
       "       -0.19326223, -0.01615027,  0.04834355, -0.05840807,  0.1450381 ,\n",
       "        0.23521996, -0.21575646, -0.24425121, -0.0681777 , -0.13122153,\n",
       "        0.05407755,  0.13657176, -0.1555095 ,  0.30131653,  0.10845796,\n",
       "        0.19657639,  0.2415795 , -0.09778815,  0.00500485,  0.15810558], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['princess'] # 위에 size를 100개로 만들었기때문에."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유비(analogy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boy', 0.9671190977096558),\n",
       " ('miller', 0.9608666896820068),\n",
       " ('cat', 0.9565996527671814),\n",
       " ('peasant', 0.9560351967811584),\n",
       " ('dwarf', 0.9556758403778076),\n",
       " ('prince', 0.9532367587089539),\n",
       " ('witch', 0.9530192017555237),\n",
       " ('wolf', 0.9506787061691284),\n",
       " ('eldest', 0.9499181509017944),\n",
       " ('shepherd', 0.9486421346664429)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['man', 'princess'], negative=['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('he', 0.3052982687950134),\n",
       " ('saw', 0.2893884479999542),\n",
       " ('laid', 0.271098256111145),\n",
       " ('So', 0.26589488983154297),\n",
       " ('When', 0.26302215456962585),\n",
       " ('found', 0.2620123624801636),\n",
       " ('heard', 0.2538025975227356),\n",
       " ('king', 0.2533321976661682),\n",
       " ('At', 0.2524864077568054),\n",
       " ('drew', 0.24930912256240845)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['man', 'queen'], negative=['woman','lady'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.984765887260437),\n",
       " ('witch', 0.9841253757476807),\n",
       " ('prince', 0.9833378195762634),\n",
       " ('wolf', 0.9830162525177002),\n",
       " ('fox', 0.9812712669372559),\n",
       " ('youngest', 0.98121577501297),\n",
       " ('third', 0.9802179336547852),\n",
       " ('dwarf', 0.9795837998390198),\n",
       " ('next', 0.9788338541984558),\n",
       " ('cook', 0.9772067070007324)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['princess'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('will', -0.5339512825012207),\n",
       " ('you', -0.5472673177719116),\n",
       " ('me', -0.5736132860183716),\n",
       " ('could', -0.5775742530822754),\n",
       " ('can', -0.5846158266067505),\n",
       " ('we', -0.586883008480072),\n",
       " ('your', -0.5908989310264587),\n",
       " ('shall', -0.5910248160362244),\n",
       " ('take', -0.5936345458030701),\n",
       " ('t', -0.5951824188232422)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(negative=['fox'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
